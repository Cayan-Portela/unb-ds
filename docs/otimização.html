<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 6 Otimização | UnB - Data Science</title>
<meta name="author" content="Cayan Portela and Herbert Kimura">
<meta name="description" content="6.1 Estatística e Machine Learning A abordagem estatística para análise e resumo de informações contidas em um conjunto de dados, consiste na suposição de que existe um mecanismo estocástico...">
<meta name="generator" content="bookdown 0.34 with bs4_book()">
<meta property="og:title" content="Chapter 6 Otimização | UnB - Data Science">
<meta property="og:type" content="book">
<meta property="og:description" content="6.1 Estatística e Machine Learning A abordagem estatística para análise e resumo de informações contidas em um conjunto de dados, consiste na suposição de que existe um mecanismo estocástico...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 6 Otimização | UnB - Data Science">
<meta name="twitter:description" content="6.1 Estatística e Machine Learning A abordagem estatística para análise e resumo de informações contidas em um conjunto de dados, consiste na suposição de que existe um mecanismo estocástico...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.5.0/transition.js"></script><script src="libs/bs3compat-0.5.0/tabs.js"></script><script src="libs/bs3compat-0.5.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">UnB - Data Science</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> About</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">2</span> Introdução</a></li>
<li><a class="" href="revis%C3%A3o.html"><span class="header-section-number">3</span> Revisão</a></li>
<li><a class="" href="probabilidade-e-estat%C3%ADstica.html"><span class="header-section-number">4</span> Probabilidade e Estatística</a></li>
<li><a class="" href="teste-de-hip%C3%B3teses.html"><span class="header-section-number">5</span> Teste de Hipóteses</a></li>
<li><a class="active" href="otimiza%C3%A7%C3%A3o.html"><span class="header-section-number">6</span> Otimização</a></li>
<li><a class="" href="other-considerations.html"><span class="header-section-number">7</span> Other Considerations</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="otimização" class="section level1" number="6">
<h1>
<span class="header-section-number">6</span> Otimização<a class="anchor" aria-label="anchor" href="#otimiza%C3%A7%C3%A3o"><i class="fas fa-link"></i></a>
</h1>
<div id="estatística-e-machine-learning" class="section level2" number="6.1">
<h2>
<span class="header-section-number">6.1</span> Estatística e Machine Learning<a class="anchor" aria-label="anchor" href="#estat%C3%ADstica-e-machine-learning"><i class="fas fa-link"></i></a>
</h2>
<p>A abordagem estatística para análise e resumo de informações contidas em um conjunto de dados, consiste na suposição de que existe um mecanismo estocástico gerador do processo em análise <span class="citation">(Bonat et al. <a href="references.html#ref-bonat2012metodos" role="doc-biblioref">2012</a>)</span>. Para isso, existem técnica que nos auxiliam na estimação de uma função (e.g. de regressão) que são de fundamental importância em estatística e machine learning.</p>
<p>Muitas ténicas de regressão usadas hoje são datadas de muitos anos atrás. Todavia, com o avanço computacional, métodos mais robustos em relação ao real processo gerador vêm ganhando cada vez mais espaço e importância<span class="citation">(Izbicki and Santos <a href="references.html#ref-izbicki2020aprendizado" role="doc-biblioref">2020</a>)</span>. Por exemplo, em modelos com mais covariáveis que observações, métodos tradicionais sofrem pela falta de graus de liberdade.</p>
<p>De maneira geral, nosso objetivo é determinar uma relação entre uma variável aleatória de interesse <span class="math inline">\(Y\)</span> e um vetor de covariáveis <span class="math inline">\(x = (x_1, ..., x_n)\)</span>. Então, temos que:</p>
<p><span class="math display">\[ g(x) := E [ Y | X = x]\]</span></p>
</div>
<div id="regressão-linear" class="section level2" number="6.2">
<h2>
<span class="header-section-number">6.2</span> Regressão Linear<a class="anchor" aria-label="anchor" href="#regress%C3%A3o-linear"><i class="fas fa-link"></i></a>
</h2>
<p><em>Regressão Linear</em> configura um dos algoritmos mais simples em aprendizado supervisionado, porém, é um modelo de extrema importância. Além de ser bastante eficaz em diversas aplicações, seu bom entendimento é resposável por fundamentos sólidos em conceitos de modelagem, pois diversas modelagens mais sofisticadas podem ser vistas como generalizações de uma regressão linear. Esta seção apresenta alguns conceitos de regressão linear. Um estudo extremamente rico compreensivo no assunto pode ser visto em <span class="citation">Kutner et al. (<a href="references.html#ref-kutner2005applied" role="doc-biblioref">2005</a>)</span>.</p>
<p>Em um modelo de regressão linear simples, temos a seguinte relação:</p>
<p><span class="math display" id="eq:um">\[\begin{align}
  y_i = \beta_0 + \beta_1 x_i + \epsilon_i \qquad  \qquad i = 1, ..., n \tag{6.1}
\end{align}\]</span></p>
<p>em que <span class="math inline">\(y_i\)</span> é a variávei de interesse (dependente), <span class="math inline">\(x_i\)</span> é a variável explicativa (independente), <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span> são os parâmetros a serem estimados pelo modelo e <span class="math inline">\(\epsilon_i\)</span> representa o erro associdao à i-ésima observação.</p>
<p>Vamos considerar um exemplo utilizando os dados <code>mtcars</code> (disponível em <code>R</code>), que consiste em informações sobre automóveis.</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th align="left"></th>
<th align="right">mpg</th>
<th align="right">cyl</th>
<th align="right">disp</th>
<th align="right">hp</th>
<th align="right">drat</th>
<th align="right">wt</th>
<th align="right">qsec</th>
<th align="right">vs</th>
<th align="right">am</th>
<th align="right">gear</th>
<th align="right">carb</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Mazda RX4</td>
<td align="right">21.0</td>
<td align="right">6</td>
<td align="right">160</td>
<td align="right">110</td>
<td align="right">3.90</td>
<td align="right">2.620</td>
<td align="right">16.46</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">4</td>
</tr>
<tr class="even">
<td align="left">Mazda RX4 Wag</td>
<td align="right">21.0</td>
<td align="right">6</td>
<td align="right">160</td>
<td align="right">110</td>
<td align="right">3.90</td>
<td align="right">2.875</td>
<td align="right">17.02</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">4</td>
</tr>
<tr class="odd">
<td align="left">Datsun 710</td>
<td align="right">22.8</td>
<td align="right">4</td>
<td align="right">108</td>
<td align="right">93</td>
<td align="right">3.85</td>
<td align="right">2.320</td>
<td align="right">18.61</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">1</td>
</tr>
</tbody>
</table></div>
<p>Quanto mais pesado o carro, menor a autonomia (quantidade de milhas percorridas por galão)?</p>
<p><span class="math display">\[Autonomia_i = \beta_0 + \beta_1 Peso_i + \epsilon_i\]</span></p>
<div class="inline-figure"><img src="bookdown-demo_files/figure-html/plot-last2-1.png" width="672"></div>
<p>Para encontrar a reta que melhor se ajusta aos pontos utilizamos a abordagem de mínimos quadrados, que consiste em encontrar estimadores <span class="math inline">\(\hat{\beta_0}\)</span> e <span class="math inline">\(\hat{\beta_1}\)</span> que minimizam a soma dos erros quadráticos.</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1 x_i + \epsilon_i\]</span>
<span class="math display">\[\epsilon_i = y_i - \beta_0 - \beta_1 x_i\]</span>
<span class="math display">\[\epsilon_i^2 = (y_i - \beta_0 - \beta_1 x_i)^2\]</span>
<span class="math display">\[\sum_{i = 1}^{n} \epsilon_i^{2} = \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2\]</span></p>
<p>Temos então, nossa função de custo, que queremos minimzar:</p>
<p><span class="math display" id="eq:dois">\[\begin{align}
  \sum_{i = 1}^{n} \epsilon_i^{2} = \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2 \tag{6.2}
\end{align}\]</span></p>
<div id="regressão-linear-simples" class="section level3" number="6.2.1">
<h3>
<span class="header-section-number">6.2.1</span> Regressão Linear Simples<a class="anchor" aria-label="anchor" href="#regress%C3%A3o-linear-simples"><i class="fas fa-link"></i></a>
</h3>
<p>Casos em que temos apenas uma covariável (x) como variável explicativa (como o exemplo acima), dizemos estar em uma regressão linear simples.</p>
<p>Para minimizar a função, aplicamos derivadas parciais, em respeito a <span class="math inline">\(\hat{\beta_0}\)</span> e <span class="math inline">\(\hat{\beta_1}\)</span>, e igualamos a zero.
Como minimizar uma função?</p>
<p>Para minimizar uma função <span class="math inline">\(f(x)\)</span>, calculamos sua derivada e igualamos a zero. Para verificar que <span class="math inline">\(f'(x) = 0\)</span> é um ponto de mínimo, devemos observar o sinal da segunda derivada, <span class="math inline">\(f''(x)\)</span>.</p>
<div class="inline-figure"><img src="images/simple_tangent_graph.png" width="40%" style="display: block; margin: auto;"></div>
<p>Fazendo as derivadas parciais da função de custo (L) em relação a <span class="math inline">\(\beta_0\)</span> e em relação a <span class="math inline">\(\beta_1\)</span>, temos:</p>
<p><span class="math display">\[L = \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2\]</span>
<span class="math display" id="eq:tres">\[\begin{align}
\frac{\partial L}{\partial \beta_0} = -2 \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i) \tag{6.3}
\end{align}\]</span></p>
<p><span class="math display" id="eq:cinco">\[\begin{align}
\frac{\partial L}{\partial \beta_1} = -2 \sum_{i=1}^{n} x_i(y_i - \beta_0 - \beta_1 x_i) \tag{6.4}
\end{align}\]</span></p>
<p>Igualando <span class="math inline">\(\frac{\partial L}{\partial \beta_0} = 0\)</span> e <span class="math inline">\(\frac{\partial L}{\partial \beta_1} = 0\)</span>, obtemos as equações:</p>
<p><span class="math display" id="eq:seis">\[\begin{align}
  \sum_{i=1}^{n} (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0  \rightarrow n \hat{\beta}_0 + \hat{\beta}_1 \sum_{i=1}^{n} x_i = \sum_{i=1}^{n} y_i \tag{6.5}
\end{align}\]</span></p>
<p><span class="math display" id="eq:sete">\[\begin{align}
  \sum_{i=1}^{n} x_i(y_i - \beta_0 - \beta_1 x_i) = 0   \rightarrow \hat{\beta}_0 \sum_{i=1}^{n} x_i + \hat{\beta}_1 \sum_{i=1}^{n} x_i^2 = \sum_{i=1}^{n} x_i y_i \tag{6.6}
\end{align}\]</span></p>
<p>De <a href="otimiza%C3%A7%C3%A3o.html#eq:seis">(6.5)</a> temos então:</p>
<p><span class="math display" id="eq:b0">\[\begin{align}
\hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x}
\tag{6.7}
\end{align}\]</span></p>
<p>Substituindo <a href="otimiza%C3%A7%C3%A3o.html#eq:b0">(6.7)</a> em <a href="otimiza%C3%A7%C3%A3o.html#eq:sete">(6.6)</a>, temos:</p>
<p><span class="math display">\[
\hat{\beta_1}  = \frac{\sum_{i=1}^{n} x_i y_i - \frac{1}{n}\sum_{i=1}^{n}x_i \sum_{i=1}^{n} y_i  }{\sum_{i=1}^{n}x^2 - \frac{1}{n} (\sum_{i=1}^{n}x_i)^2} = 
\frac{n \sum_{i=1}^{n} x_i y_i - \sum_{i=1}^{n} x_i \sum_{i=1}^{n} y_i}{n \sum_{i=1}^{n} x_i^2 - (\sum_{i=1}^{n} x_i)^2}
\]</span></p>
<p><span class="math display" id="eq:b1">\[\begin{align}
\hat{\beta_1} = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^2} \tag{6.8}
\end{align}\]</span></p>
<hr>
<p>Exercício. Fazer “na mão”, comparando com o resultado da função lm().</p>
</div>
<div id="forma-matricial" class="section level3" number="6.2.2">
<h3>
<span class="header-section-number">6.2.2</span> Forma Matricial<a class="anchor" aria-label="anchor" href="#forma-matricial"><i class="fas fa-link"></i></a>
</h3>
<p>O modelo visto acima pode ser representado matricialmente, generalizando para o caso em que temos mais de uma variável explicativa.</p>
<p><span class="math display">\[y_1 = \beta_0 + \beta_1 x_1 + \epsilon_1\]</span>
<span class="math display">\[y_2 = \beta_0 + \beta_1 x_2 + \epsilon_2\]</span>
<span class="math display">\[\vdots\]</span>
<span class="math display">\[y_n = \beta_0 + \beta_1 x_n + \epsilon_n\]</span></p>
<p><span class="math display">\[\begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n  \end{bmatrix} = \begin{bmatrix} 1 &amp; x_{11}\\ 1 &amp; x_{12} \\ \vdots &amp; \vdots \\ 1 &amp; x_{1n}  \end{bmatrix} \times \begin{bmatrix} \beta_0 \\ \beta_1 \\  \end{bmatrix} + \begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n  \end{bmatrix}\]</span></p>
<p><span class="math display">\[\Large \boldsymbol{Y} = \boldsymbol{XB} + \boldsymbol{\epsilon}\]</span></p>
<p>Como estimar os parâmetros com a representação matricial? Mesma idéia: minimizar os erros!</p>
<p><span class="math display">\[\boldsymbol{Y} = \boldsymbol{X\beta} + \boldsymbol{\epsilon}\]</span>
<span class="math display">\[\boldsymbol{\epsilon} = \boldsymbol{Y} - \boldsymbol{X\beta}\]</span></p>
<p>Mínimos quadrados: como representar soma quadrática dos erros matricialmente?
<span class="math display">\[\boldsymbol{\epsilon}' \boldsymbol{\epsilon}\]</span>
Então,</p>
<p><span class="math display">\[\boldsymbol{\epsilon}' \boldsymbol{\epsilon} = (\boldsymbol{Y} - \boldsymbol{X\beta})'(\boldsymbol{Y} - \boldsymbol{X\beta})\]</span></p>
<p>Novamente: mesma ideia, derivar e igualar a zero.</p>
<p><span class="math display">\[\boldsymbol{Y} = \boldsymbol{X\beta} + \boldsymbol{\epsilon}\]</span>
<span class="math display">\[\boldsymbol{\epsilon} = \boldsymbol{Y} - \boldsymbol{X\beta}\]</span>
<span class="math display">\[\boldsymbol{\epsilon}' \boldsymbol{\epsilon} = (\boldsymbol{Y} - \boldsymbol{X\beta})'(\boldsymbol{Y} - \boldsymbol{X\beta})\]</span>
<span class="math display">\[= (\boldsymbol{Y} - \boldsymbol{X\beta})'(\boldsymbol{Y} - \boldsymbol{X\beta})\]</span>
<span class="math display">\[= (\boldsymbol{Y'} - \boldsymbol{\beta'X'})(\boldsymbol{Y} - \boldsymbol{X\beta})\]</span>
<span class="math display">\[= \boldsymbol{Y'}Y - \boldsymbol{\beta'X'Y} - \boldsymbol{Y'X\beta} + \boldsymbol{\beta'X'X\beta}\]</span>
<span class="math display">\[= \boldsymbol{Y'Y} - 2\boldsymbol{\beta'X'Y}  + \boldsymbol{\beta'X'X\beta}\]</span>
<span class="math display">\[-2\boldsymbol{X'Y}  + 2\boldsymbol{X'X\beta} = 0\]</span>
<span class="math display">\[\boldsymbol{X'X\beta} = \boldsymbol{X'Y}\]</span>
<span class="math display">\[\boldsymbol{\hat{\beta}} = \boldsymbol{(X'X)^{-1}X'Y}\]</span></p>
</div>
<div id="decomposição-qr" class="section level3" number="6.2.3">
<h3>
<span class="header-section-number">6.2.3</span> Decomposição QR<a class="anchor" aria-label="anchor" href="#decomposi%C3%A7%C3%A3o-qr"><i class="fas fa-link"></i></a>
</h3>
<p>Operações envolvendo a inversa de uma matriz podem ser bastante custosas do ponto de vista computacional, principalmente quando a matriz cresce em dimensões. Uma alternativa bastante utilizada é a decomposição <span class="math inline">\(QR\)</span> (através do algoritmo Gram-Schmidt, por exemplo).</p>
<p>Sendo <span class="math inline">\(X_{n\times p}\)</span> uma matriz de posto completo (<span class="math inline">\(n\)</span> &gt; <span class="math inline">\(p\)</span>, rank=<span class="math inline">\(p\)</span>), então podemos decompor <span class="math inline">\(\boldsymbol{X}\)</span> em <span class="math display">\[\boldsymbol{X} = \boldsymbol{QR}\]</span></p>
<p>em que:</p>
<ul>
<li>
<span class="math inline">\(\boldsymbol{Q}\)</span> tem dimensões <span class="math inline">\(n\times p\)</span> e <span class="math inline">\(\boldsymbol{Q'Q} = \boldsymbol{I}\)</span>
</li>
<li>
<span class="math inline">\(\boldsymbol{R}\)</span> tem dimensões <span class="math inline">\(p \times p\)</span> e é uma matriz triangular superior</li>
</ul>
<p>Dessa maneira, podemos reescrever o processo de mínimos quadrados acima, como:</p>
<p><span class="math display">\[\begin{align}
&amp;\boldsymbol{X'X\beta} &amp;\ = \quad &amp;\boldsymbol{X'Y} \\
&amp;\boldsymbol{(QR)'(QR)\beta} &amp;\ = \quad  &amp;\boldsymbol{(QR)'Y} \\
&amp;\boldsymbol{R'R\beta} &amp;\ = \quad  &amp;\boldsymbol{R'Q'Y} \\
&amp;\boldsymbol{(R')^{-1}R'R\beta} &amp;\ = \quad &amp; \boldsymbol{(R')^{-1}R'Q'Y} \\
&amp;\boldsymbol{R\beta} &amp;\ = \quad &amp; \boldsymbol{Q'Y} \\
\end{align}\]</span></p>
<p><span class="math display">\[\boldsymbol{R\beta} = \boldsymbol{Q'Y}\]</span></p>
<p>Dessa maneira, podemos chegar nos valores preditos <span class="math inline">\(\hat{y}\)</span> através de <span class="math inline">\(\boldsymbol{\hat{Y}} = \boldsymbol{QRR^{-1}Q'} = \boldsymbol{QQ'Y}\)</span></p>
<blockquote>
<p><strong><em>Exercício</em></strong>: adicione colinearidade na regressão dos dados mtcars e compare o resultado da função <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> no R e <code>sklearn.linear_model.LinearRegression()</code> do python.</p>
</blockquote>
</div>
</div>
<div id="gradiente-descendente" class="section level2" number="6.3">
<h2>
<span class="header-section-number">6.3</span> Gradiente Descendente<a class="anchor" aria-label="anchor" href="#gradiente-descendente"><i class="fas fa-link"></i></a>
</h2>
<p>Gradiente descendente é um dos algoritmos de otimização mais populares e amplamente utilizado, principalmente em otimização de redes neurais. O algoritmo busca encontrar o mínimo de uma função <span class="math inline">\(J(\beta)\)</span> de forma iterativa, em que <span class="math inline">\(\beta\)</span> são os parâmetros do modelo. A ideia consiste em atualizar os parâmetros na direção oposta do gradiente <span class="math inline">\(\nabla_{\beta} J(\beta)\)</span> em relação aos parâmetros <span class="citation">(Ruder <a href="references.html#ref-ruder2016overview" role="doc-biblioref">2016</a>)</span>, ou seja, seguimos a direção da inclinação da região de resposta criada, em busca de um vale. Em sua forma mais simples, o algoritmo é definido como:</p>
<p><span class="math display" id="eq:gradientedesc">\[\begin{align}
\beta^{(k+1)} = \beta^{(k)} - \alpha_k \nabla J(\beta^{k}), \qquad k = 0, 1, \cdots
\tag{6.9}
\end{align}\]</span></p>
<p>Em que <span class="math inline">\(\beta^{(k)}\)</span> são os valores do vetor de parâmetros no passo <span class="math inline">\(k\)</span>, <span class="math inline">\(\alpha\)</span> é a taxa de aprendizado (<em>learning rate</em>) e <span class="math inline">\(\nabla_{\beta} J(\beta)\)</span> é o gradiente da função de custo <span class="math inline">\(L(\cdot)\)</span>, ou seja, o vetor de derivadas parciais em relação aos parâmetros <span class="math inline">\(\beta\)</span>. A taxa de aprendizado <span class="math inline">\(\alpha\)</span> determina o tamanho do passo em cada atualização, visando atingir o mínimo (local).</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:gradient-descent-fig"></span>
<img src="bookdown-demo_files/figure-html/gradient-descent-fig-1.png" alt="Gradiente descendente minimiza a função de custo gradualmente, ajustando o(s) parâmetro(s) em cada passo (figura adaptada de Boehmke &amp; Greenwell, 2019)." width="672"><p class="caption">
Figure 6.1: Gradiente descendente minimiza a função de custo gradualmente, ajustando o(s) parâmetro(s) em cada passo (figura adaptada de Boehmke &amp; Greenwell, 2019).
</p>
</div>
<p>Com uma taxa de aprendizado muito pequena são necessários muitas iterações para encontrar o mínimo. Todavia, caso a taxa de aprendizado seja muito grande, você pode acabar “pulando” o ponto de mínimo desejado.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:gradient-descent-step"></span>
<img src="bookdown-demo_files/figure-html/gradient-descent-step-1.png" alt="Taxa de aprendizado pequena e grande (figura adaptada de Boehmke &amp; Greenwell, 2019)." width="672"><p class="caption">
Figure 6.2: Taxa de aprendizado pequena e grande (figura adaptada de Boehmke &amp; Greenwell, 2019).
</p>
</div>
<p>Vamos ilustrar utilizando o exemplo acima. Regressão Linear.</p>
<p><span class="math display">\[\hat{L}(f_\beta) = \frac{1}{n} \sum_{i=1}^{n} (x\beta - y)^2 =  \frac{1}{n} || X\beta - Y||_2^2\]</span></p>
<p>O gradiente é igual a:</p>
<p><span class="math display">\[\begin{align}
 &amp;\nabla_\beta \hat{L}(f_\beta) &amp;=\\
 &amp; \nabla_\beta \frac{1}{n} || X\beta - Y||_2^2 &amp;=\\
 &amp; \nabla_\beta [(X\beta - Y)'(X\beta - Y)] &amp;=\\
 &amp; \nabla_\beta [(\beta'X' - Y')(X\beta - Y)] &amp;=\\
 &amp; \nabla_\beta [\beta'X'X\beta - 2\beta'X'Y + Y'Y] &amp;=\\
 &amp; 2X'X\beta - 2X'Y &amp;=\\
 &amp; 2X'( X\beta - Y)
\end{align}\]</span></p>
</div>
<div id="regressão-logística" class="section level2" number="6.4">
<h2>
<span class="header-section-number">6.4</span> Regressão Logística<a class="anchor" aria-label="anchor" href="#regress%C3%A3o-log%C3%ADstica"><i class="fas fa-link"></i></a>
</h2>
<p>Para problemas de classificação binária, nossa variável Y assume dois possíveis valores: 0 ou 1 (sim ou não). Assim, desejamos saber a probabilidade de y assumir o valor target 1. Para isso, o modelo de Regerssão Logística utiliza a função de ligação logística, como função de ligação na combinação linear do modelo. Agora estamos modelando uma probabilidade, então nossa variável target <span class="math inline">\(Y = p(X)\)</span> indica a probabilidade de ocorrência do evento.</p>
<p>Então, temos que:</p>
<p><span class="math display">\[\begin{align}
&amp;p(X) &amp;\ = \quad &amp;\frac{e^{\beta_0 + \beta_1x}}{1 + e^{\beta_0 + \beta_1x}} \\
&amp;\frac{p(X)}{1-p(x)} &amp;\ = \quad &amp;e^{\beta_0 + \beta_1x}
\end{align}\]</span></p>
<p>Então:</p>
<p><span class="math display" id="eq:reglogistica">\[\begin{align}
&amp; \text{log}\left(\frac{p(X)}{1-p(x)}\right) = {\beta_0 + \beta_1x}
\tag{6.10}
\end{align}\]</span></p>
<p>O processo de otimização para encontrar os estimadores dos parâmetros <span class="math inline">\(\boldsymbol{\beta}\)</span> envolve maximizar a log-verossimilhança da distribuição <span class="math inline">\(Bernoulli(p)\)</span>.</p>
<blockquote>
<p><span class="math inline">\(\text{Bernoulli}(p):\)</span>
Distribuição associada a um único experimento aleatório com apenas dois resultados possíveis: <span class="math inline">\(0\)</span>, caso se observe <span class="math inline">\(A\)</span>; e <span class="math inline">\(1\)</span>, caso se observe <span class="math inline">\(A^c\)</span>.
Seja <span class="math inline">\(X=\)</span> número de sucessos (espaço amostral finito), então temos que:</p>
</blockquote>
<p><span class="math display">\[f(x)=P(X=x)=\begin{cases} 1-p,&amp;\quad x=0,\\ p,&amp;\quad x=1.\end{cases}\]</span></p>
<p><br></p>
<p><span class="math display">\[\begin{align}
&amp; L(\beta) &amp;\ = &amp;\prod p_i^{y_i} \times (1-p_i)^{(1-y_i)} \\
&amp; \text{log}L(\beta) &amp;\ = &amp;\sum y_i \text{log}(p_i) + (1-y_i)\text{log}(1-p_i) \\
&amp; \text{log}L(\beta) &amp;\ = &amp;\sum y_i \text{log}(p_i) + \text{log}(1-p_i) - y_i\text{log}(1-p_i) \\
&amp; \text{log}L(\beta) &amp;\ = &amp;\sum y_i \text{log}(p_i) - y_i\text{log}(1-p_i) + \text{log}(1-p_i) \\
&amp; \text{log}L(\beta) &amp;\ = &amp;\sum y_i \text{log}\left(\frac{p_i}{1-p_i}\right) + \text{log}(1-p_i) \\
&amp; \text{log}L(\beta) &amp;\ = &amp;\sum y_i \text{log}\left(\frac{p_i}{1-p_i}\right) + \text{log}\left(\frac{1}{1+e^{\beta_0 + \beta_1x}}\right)
\end{align}\]</span></p>
<p><br></p>
<p><span class="math display">\[\begin{align}
&amp; \text{log}L(\beta) &amp; = \sum y_i \text{log}(\frac{p_i}{1-p_i}) + \text{log}(1) - \text{log}(1+e^{\beta_0 + \beta_1x})
\end{align}\]</span></p>
<p>Substituindo <span class="math inline">\(\text{log}\left(\frac{p(X)}{1-p(x)}\right) = {\beta_0 + \beta_1x}\)</span>, obtemos a função de <span class="math inline">\(\boldsymbol{\beta}\)</span> que desejamos otimizar:</p>
<p><span class="math display" id="eq:verosslogistica">\[\begin{align}
\sum_{i=1}^n   \left[ y_i \boldsymbol{\beta}^T  \mathbf{x}_{i}  - \log \left(1 + \exp(  \boldsymbol{\beta}^T \mathbf{x}_{i}  \right)    \right]
\tag{6.11}
\end{align}\]</span></p>
<div id="newton-raphson" class="section level3" number="6.4.1">
<h3>
<span class="header-section-number">6.4.1</span> Newton-Raphson<a class="anchor" aria-label="anchor" href="#newton-raphson"><i class="fas fa-link"></i></a>
</h3>
<p>Neste caso não possuimos uma solução fechada para <span class="math inline">\({\bf \beta}\)</span>. O método Newton-Raphson é um método iterativo que pode ser usado para estimar o vetor de parâmetros <span class="math inline">\({\bf \beta}\)</span>. Em cada iteração, fazemos uma aproximação quadrática do log de verossimilhança e maximizamos isso.</p>
<p>Mais especificamente, seja <span class="math inline">\(u \in \mathbb{R}^p\)</span> o gradiente <span class="math inline">\(\partial L / \partial \beta\)</span>, <span class="math inline">\({\bf H} \in \mathbb{R}^{p \times p}\)</span> a matriz Hessiana, e assuma que <span class="math inline">\(\beta^{(k)}\)</span> é nossa estimativa para <span class="math inline">\(\hat{\beta}\)</span> no tempo <span class="math inline">\(k\)</span>. (Podemos pensar em <span class="math inline">\(u\)</span> e <span class="math inline">\({\bf H}\)</span> como funções de <span class="math inline">\(\beta\)</span>). Sejam <span class="math inline">\(u^{(t)}\)</span> e <span class="math inline">\({\bf H}^{(k)}\)</span> a avaliação de <span class="math inline">\(u\)</span> e <span class="math inline">\(\bf H\)</span> em <span class="math inline">\(\beta^{(k)}\)</span>. A expansão de Taylor de <span class="math inline">\(L\)</span> em torno de <span class="math inline">\(\beta^{(k)}\)</span> dá</p>
<span class="math display">\[\begin{aligned} L(\beta) \approx L(\beta^{(t)}) + (u^{(t)})^T (\beta - \beta^{(t)}) + \frac{1}{2} (\beta - \beta^{(t)})^T {\bf H}^{(t)} (\beta - \beta^{(t)}). \end{aligned}\]</span>
<p>Derivando em relação à <span class="math inline">\(\beta\)</span> e definindo-o como 0, podemos reorganizar os termos para resolver <span class="math inline">\(\beta\)</span>, dando-nos a próxima estimativa</p>
<p>O vetor score <span class="math inline">\({\bf u}\)</span> e a matriz Hessiana <span class="math inline">\({\bf H}\)</span> representam as derivadas de primeira e segunda ordem, respectivamente, em que:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial l}{\partial \beta}  =\boldsymbol{X}^T\boldsymbol{(y - p)}\\
%\frac{\partial ^2 \ell}{\partial \beta^2} = - \sum_{i=1}^n \mathbf{x}_{i} \mathbf{x}_{i}^T p_i (1-p_i) \\
\frac{\partial ^2 l}{\partial \beta^2} = \boldsymbol{-X^TWX } 
\end{align}\]</span></p>
<p>em que <span class="math inline">\(\boldsymbol{W}\)</span> é uma matriz diagonal <span class="math inline">\(n \times n\)</span> com o i-ésimo elemento dado por <span class="math inline">\(p_i(1-p_i)\)</span>, ou seja, <span class="math inline">\(W_{ii} = p_i (1-p_i)\)</span>, com <span class="math inline">\(1 \le i \le n\)</span>.
. Dessa maneira, o algoritmo pode ser reescrito como:</p>
<span class="math display">\[\begin{aligned} \beta^{(t+1)} = \beta^{(t)} - ({\bf H}^{(t)})^{-1} u^{(t)}. \end{aligned}\]</span>
<p><span class="math display">\[\begin{align}
\boldsymbol{\beta}^{(k+1)} = \boldsymbol{\beta}^{(k)} - \left(  \frac{\partial ^2 l}{\partial \beta^2} \right) \frac{\partial l}{\partial \beta}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\boldsymbol{\beta}^{(k+1)} = \boldsymbol{\beta}^{(k)} + (\mathbf{X}^T\mathbf{WX})^{-1}\mathbf{X}^T\mathbf{(y-p)} 
\end{align}\]</span></p>

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="teste-de-hip%C3%B3teses.html"><span class="header-section-number">5</span> Teste de Hipóteses</a></div>
<div class="next"><a href="other-considerations.html"><span class="header-section-number">7</span> Other Considerations</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#otimiza%C3%A7%C3%A3o"><span class="header-section-number">6</span> Otimização</a></li>
<li><a class="nav-link" href="#estat%C3%ADstica-e-machine-learning"><span class="header-section-number">6.1</span> Estatística e Machine Learning</a></li>
<li>
<a class="nav-link" href="#regress%C3%A3o-linear"><span class="header-section-number">6.2</span> Regressão Linear</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#regress%C3%A3o-linear-simples"><span class="header-section-number">6.2.1</span> Regressão Linear Simples</a></li>
<li><a class="nav-link" href="#forma-matricial"><span class="header-section-number">6.2.2</span> Forma Matricial</a></li>
<li><a class="nav-link" href="#decomposi%C3%A7%C3%A3o-qr"><span class="header-section-number">6.2.3</span> Decomposição QR</a></li>
</ul>
</li>
<li><a class="nav-link" href="#gradiente-descendente"><span class="header-section-number">6.3</span> Gradiente Descendente</a></li>
<li>
<a class="nav-link" href="#regress%C3%A3o-log%C3%ADstica"><span class="header-section-number">6.4</span> Regressão Logística</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#newton-raphson"><span class="header-section-number">6.4.1</span> Newton-Raphson</a></li></ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>UnB - Data Science</strong>" was written by Cayan Portela and Herbert Kimura. It was last built on 2023-09-12.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
