[{"path":"index.html","id":"about","chapter":"1 About","heading":"1 About","text":"Authors: Cayan Portela Herbet Kimura.University Brasília, Mangement Department.material constant development used show fundamental concepts data science.Subjects:\nProbability Statistics.\nQuantitative Finance.\nProbability Statistics.Quantitative Finance.","code":""},{"path":"intro.html","id":"intro","chapter":"2 Introduction","heading":"2 Introduction","text":"Reference figure code chunk label fig: prefix, e.g., Similarly, can reference tables generated knitr::kable(), e.g., see Table .can write citations, . example, using bookdown package (Xie 2023) sample book, built top R Markdown knitr (Xie 2015).","code":""},{"path":"revisão.html","id":"revisão","chapter":"3 Revisão","heading":"3 Revisão","text":"Nesta seção revisaremos brevemente alguns conceitos relacionados ao nosso curso.","code":""},{"path":"revisão.html","id":"álgebra-linear-alguns-conceitos.","chapter":"3 Revisão","heading":"3.1 Álgebra Linear: alguns conceitos.","text":"","code":""},{"path":"revisão.html","id":"matriz","chapter":"3 Revisão","heading":"3.1.1 Matriz","text":"Matriz é um arranjo retangular (duas dimensões),de números ou variáveis, em linhas e colunas.\\[ \\boldsymbol{} = \\begin{bmatrix}10 & 12\\\\21 & 39\\end{bmatrix}\\]\n\\[ \\boldsymbol{B} = \\begin{bmatrix}1 & 1 & 1 & 1 & 1 & 1\\\\10 & 12 & 15 & 13 & 14 & 16\\end{bmatrix}\\]Para representar os elements da matriz como variáveis, usamos o índice linha por coluna. Dessa maneira, \\(x_{ij}\\) representa o elemento \\(x\\) correspondente à \\(\\)-ésima linha e \\(j\\)-ésima coluna:\\[ \\boldsymbol{X} = (x_{ij}) =\\begin{bmatrix}x_{11} & x_{12} & x_{13}\\\\x_{21} & x_{22} & x_{23}\\\\x_{31} & x_{32} & x_{33}\\\\x_{41} & x_{42} & x_{43}\\end{bmatrix}\\]Usualmente, denotamos uma matriz genérica por \\(n\\) linhas e \\(p\\) colunas. matriz acima, poussui \\(n = 4\\) linhas e \\(p = 3\\) colunas.Um vetor é uma matriz com uma dimensão igual 1. Geralmente, o termo vetor esa associado um vetor coluna. Um vetor linha é representado como o transposto vetor coluna.\\[y' = y^t =\\begin{bmatrix}y_1 & y_2 & y_3\\end{bmatrix}\\]Duas matrizes (ou vetores) são equivalentes se possuem mesma dimensão e todos os elementos são correspondentes.\n\\[\\begin{bmatrix}3 & -2 & 4\\\\7 & 1 & 3\\end{bmatrix} \\neq \\begin{bmatrix}3 & -2 & 4\\\\7 & -4 & 3\\end{bmatrix}\\]\n\\[\\begin{bmatrix}3 & -2 & 4\\\\7 & 1 & 3\\end{bmatrix} = \\begin{bmatrix}3 & -2 & 4\\\\7 & 1 & 3\\end{bmatrix}\\]","code":""},{"path":"revisão.html","id":"matriz-transposta","chapter":"3 Revisão","heading":"3.1.2 Matriz Transposta","text":"Uma matriz transposta é dada pela troca de posição entre linhas e colunas, ou seja, linhas se tornam colunas e colunas se tornam linhas. Denotamos por \\(\\boldsymbol{'}\\) ou \\(\\boldsymbol{}^t\\). Formalmente, se \\(\\boldsymbol{A_{n \\times p}} = a_{ij}\\), sua transposta é dada por:\\[\\boldsymbol{'_{p \\times n}} = ^t = (a_{ij})' = (a_{ji})\\]\\[= \\begin{bmatrix}3 & -2 & 4\\\\ 7 & 1 & 3\\end{bmatrix} \\rightarrow ' = \\begin{bmatrix}3 & 7\\\\ -2 & 1\\\\ 4 & 3\\end{bmatrix}\\]","code":"\nmat2 <- matrix( c(3, -2, 4,\n                  7, 1, 3),\n                byrow = TRUE, ncol = 3)\nmat2     [,1] [,2] [,3]\n[1,]    3   -2    4\n[2,]    7    1    3\ndim(mat2)[1] 2 3"},{"path":"revisão.html","id":"matriz-simétrica","chapter":"3 Revisão","heading":"3.1.3 Matriz Simétrica","text":"Se matriz transposta é quivalente à matriz original, ou seja, \\(' = \\), então dizemos que matriz \\(\\) é uma matriz simétrica. Toda matriz simétrica é quadrada (número de linhas = número de colunas). Em uma matriz simétrica, \\(a_{ij} = a_{ji}\\)\\[= \\begin{bmatrix}3 & 8 & 4\\\\ 8 & 5 & 9\\\\ 4 & 9 & 7\\end{bmatrix}\\]","code":"\nmat3 <- matrix( c(3, 8, 4,\n                  8, 5, 9,\n                  4, 9, 7), ncol = 3)\n\nmat3[1, 2] == mat3[2, 1][1] TRUE"},{"path":"revisão.html","id":"matriz-diagonal","chapter":"3 Revisão","heading":"3.1.4 Matriz Diagonal","text":"Todos os elementos fora da diagonal de uma matriz são zero.\n\\[= \\begin{bmatrix}3 & 0 & 0\\\\ 0 & 5 & 0\\\\ 0 & 0 & 7\\end{bmatrix}\\]Com função diag() podemos criar uma matriz diagonal ou obter diagonal de uma amtriz","code":"\ndiag(c(3, 5, 7))     [,1] [,2] [,3]\n[1,]    3    0    0\n[2,]    0    5    0\n[3,]    0    0    7\ndiag(mat3)[1] 3 5 7"},{"path":"revisão.html","id":"operações-com-matrizes","chapter":"3 Revisão","heading":"3.1.5 Operações com Matrizes","text":"Adição de matrizes é feita somando seus elementos correspondentes.\n\\[\\begin{bmatrix}7 & -3 & 4\\\\ 2 & 8 & 5\\end{bmatrix} + \\begin{bmatrix}11 & 5 & 1\\\\ 3 & -2 & 0\\end{bmatrix} = \\begin{bmatrix}18 & 2 & 5\\\\ 5 & 6 & 5\\end{bmatrix}\\]O produto \\(\\boldsymbol{AB}\\) de duas matrizes só é possível se matrizes possuem dimensões compatíveis. O número de coluna da matriz \\(\\boldsymbol{}\\) deve ser o mesmo que o número de linhas da matriz \\(\\boldsymbol{B}\\). Dizemos então, que matrizes \\(\\boldsymbol{}\\) e \\(\\boldsymbol{B}\\) são conformes. O elemento \\(AB_{11}\\) da matriz resultado, é dado pela soma ponderada entre linha 1 e coluna 1. exemplo, \\(AB_{11} = (2 \\times 11) + (1 \\times 3) + (3 \\times 4) = 37\\).\\[= \\begin{bmatrix}2 & 1 & 3\\\\ 4 & 6 & 5\\end{bmatrix} \\hskip1em \\text{e} \\hskip1em B = \\begin{bmatrix}11 & 5\\\\ 3 & -2\\\\ 4 & 1\\end{bmatrix}\\]R, utilizamos o operador %*% para realizar multiplicações matriciais.matriz identidade é o elemento neutro da multiplicação de matrizes.\\[AB = \\begin{bmatrix}37 & 11\\\\ 82 & 13\\end{bmatrix} \\times \\begin{bmatrix}1 & 0\\\\ 0 & 1\\end{bmatrix} = \\begin{bmatrix}37 & 11\\\\ 82 & 13\\end{bmatrix}\\]Note que uma multiplicação entre um vetor \\(v_{1\\times n}\\) e um vetor \\(e_{n\\times 1}\\) resulta em um escalar.\n\\[\\begin{bmatrix}2 & 3 & 5 & 1\\end{bmatrix} \\times \\begin{bmatrix}1\\\\ 2\\\\ 4 \\\\ 2\\end{bmatrix} =30\\]Desta maneira, multiplicação de um vetor transposto, por ele mesmo, é sua soma quadrática.\\['= \\begin{bmatrix}2 & 3 & 5 & 1\\end{bmatrix} \\times \\begin{bmatrix}2\\\\ 3\\\\ 5 \\\\ 1\\end{bmatrix} = \\sum_{=1}^4 a_i^2\\]\n\\[\\sum_{=1}^4 a_i^2 = 2^2 + 3^2 + 5^2 + 1^2 = 39\\]Da mesma forma, multiplicação de um vetor pelo seu transposto, resulta em uma matriz quadrada e simétrica.\\[aa' = \\begin{bmatrix}2\\\\ 3\\\\ 5\\\\ 1\\end{bmatrix} \\times \\begin{bmatrix}2 & 3 & 5 & 1\\end{bmatrix} = \\begin{bmatrix}4 & 6 & 10 & 2\\\\6 & 9 & 15 & 3\\\\10 & 15 & 25 & 5\\\\2 & 3 & 5 & 1\\end{bmatrix}\\]","code":"\nmat4 <- matrix( c(7, -3, 4,\n                  2, 8, 5), byrow = TRUE, ncol = 3)\n\nmat5 <- matrix( c(11, 5, 1,\n                  3, -2, 0), byrow = TRUE, ncol = 3)\nmat4 + mat5     [,1] [,2] [,3]\n[1,]   18    2    5\n[2,]    5    6    5\nmat6 <- matrix( c(2, 1, 3, 4, 6, 5), byrow = TRUE, ncol = 3)\nmat7 <- matrix( c(11, 5, 3, -2, 4, 1), byrow = TRUE, ncol = 2)\nAB = mat6 %*% mat7\nAB     [,1] [,2]\n[1,]   37   11\n[2,]   82   13\nAB %*% diag(2)     [,1] [,2]\n[1,]   37   11\n[2,]   82   13\nt(c(2, 3, 5, 1)) %*% c(1, 2, 4, 2)     [,1]\n[1,]   30\nvetor <- c(2, 3, 5, 1)\n\nt(vetor) %*% vetor     [,1]\n[1,]   39"},{"path":"revisão.html","id":"posto-rank-de-uma-matriz","chapter":"3 Revisão","heading":"3.1.6 Posto (rank) de uma Matriz","text":"O posto (rank) de uma matriz \\(\\) está relacionado ao conceito de independência linear.Definição: o posto (rank) de uma matriz \\(\\) é definido como o número de colunas (linhas) linearmente independentes de .Uma matriz \\(A_{n \\times p}\\), em que \\(p<n\\), é dita de posto completo, caso tenha posto \\(p\\).","code":""},{"path":"revisão.html","id":"inversa-de-uma-matriz","chapter":"3 Revisão","heading":"3.1.7 Inversa de uma Matriz","text":"Uma matriz quadrada de posto completo é dita não-singular. Isso significa que ela possui inversa única.\n\\[= \\begin{bmatrix}& b\\\\ c & d\\end{bmatrix} \\hskip1em ; \\hskip1em \\rightarrow \\hskip1em ^{-1} = \\frac{1}{\\text{det}} \\begin{bmatrix}d & -b\\\\ -c  & \\end{bmatrix}\\]\n\\[= \\begin{bmatrix}4 & 7\\\\ 2 & 6\\end{bmatrix} \\hskip1em ; \\hskip1em \\rightarrow \\hskip1em ^{-1} = \\frac{1}{ 24 - 14 } \\begin{bmatrix}6 & -7\\\\ -2  & 4\\end{bmatrix}\\]R, calculamos inversa de uma matriz através da função solve()","code":"\nmat8 = matrix(c(4,7,2,6), byrow = T, ncol = 2)\nsolve(mat8)     [,1] [,2]\n[1,]  0.6 -0.7\n[2,] -0.2  0.4"},{"path":"methods.html","id":"methods","chapter":"4 Methods","heading":"4 Methods","text":"describe methods chapter.Math can added body using usual syntax like ","code":""},{"path":"methods.html","id":"lei-dos-grandes-números","chapter":"4 Methods","heading":"4.1 Lei dos Grandes Números","text":"Em Teoria de Probabilidade, Lei dos Grandes Números descreve que o resultado ao realizar o mesmo experimento um grande número de vezes.\nDe acordo com lei, média dos resultados obtidos em um grande número de tentativas deve estar próxima valor esperado, e tende se aproximar mais, à medida que mais tentativas são realizadas.\nSeja \\(X_1, X_2, ..., X_N\\) uma sequência de variáveis aleatórias independentes de uma mesma distribuição de probabilidade. Seja \\(E(X_i) = \\mu\\). Então, com probabilidade 1, temos que:\\[\n\\frac{X_1 + X_2 + X_3 + \\cdots + X_n}{n} \\rightarrow \\mu \\quad \\quad \\quad \\text{} \\quad n \\rightarrow \\infty\n\\]Para ilustrar o teorema acima, suponha que, probabilidade de conclusão de um curso, em cada turma de 30 alunos, seja \\(p = 0.7\\). Uma turma selecioanda aleatoriamente, possui o seguinte número de formandos:Imagine agora, que coletamos mesma informação, para 100 turmas.Vamos olhar média das três primeiras turmas:Mas se juntarmos, todos os alunos, em uma única gigantesca turma, temos seguinte proporção de formandos:","code":"\nturma_1 <- rbinom(n = 30, size = 1, prob = 0.7)\nsum(turma_1)[1] 22\nmean(turma_1)[1] 0.7333333\nn_turmas <- vector(mode = \"list\", length = 100)\n\nfor (i in seq_along(n_turmas)){\n    n_turmas[[i]] <- rbinom(n = 30, size = 1, prob = 0.7)\n}\nn_turmas[[1]] %>% mean()[1] 0.7333333\nn_turmas[[2]] %>% mean()[1] 0.7333333\nn_turmas[[3]] %>% mean()[1] 0.7666667\nn_turmas %>% unlist() %>% mean()[1] 0.697"},{"path":"methods.html","id":"teorema-do-limite-central","chapter":"4 Methods","heading":"4.2 Teorema do Limite Central","text":"Seja \\(X_1, X_2, ..., X_N\\) uma sequência de variáveis aleatórias independentes e identicamente distribuídas (..d), com média \\(\\mu\\) e variância \\(\\sigma^2\\).\ndistribuição de\\[\n\\frac{X_1 + X_2 + X_3 + \\cdots + X_n - n\\mu}{\\sigma \\sqrt{n}} \\rightarrow Z \\sim N(0,1) \\quad \\quad \\quad \\text{} \\quad n \\rightarrow \\infty\n\\]Então temos que:\n\\[\n\\frac{\\bar{X} - \\mu}{ \\frac{\\sigma}{\\sqrt{n}} } \\rightarrow Z \\sim N(0,1) \\quad \\quad \\quad \\text{} \\quad n \\rightarrow \\infty\n\\]Em outras palavras, o Teorema Limite Central garante que, para grandes amostras, média amostral padronizada segue uma distribuição normal com média 0 e variância 1.\n","code":""},{"path":"teste-de-hipóteses.html","id":"teste-de-hipóteses","chapter":"5 Teste de Hipóteses","heading":"5 Teste de Hipóteses","text":"Em situações em que estamos interessados em testar afirmações sobre determinado parâmetro desconhecido. O parâmetro representa uma medida de interesse estudo, levando à formulação de uma hipótese.\nCom hipótese formulada, utilizamos metodologias estatísticas para nos auxiliar em tomadas de decisões.","code":""},{"path":"teste-de-hipóteses.html","id":"alguns-conceitos","chapter":"5 Teste de Hipóteses","heading":"5.1 Alguns Conceitos","text":"População: conjunto de observações de interesse. Em geral, não temos acesso à todos os elementos da população. (recursos escassos ou população infinita)Dessa maneira, o conhecimento adquirido em relação à população de interesse é feito exaimando apenas alguns elementos da população.Amostra Aleatória Simples (..S.): Nesse tipo de amostragem, cada elemento tem mesma probabilidade de ser incluído na amostra.Ao selecionar uma amostra aleatória de tamanho \\(n\\), obtemos um vetor aleatório \\((x_1, x_2, ..., x_n)\\). Com o vetor aleatória, podemos calcular média amostral \\(\\bar{x}\\).\nmédia amostral \\(\\bar{x}\\) também é uma variável aleatória, pois seu valor varia de amostra para amostra. medidas geradas por valores amostrais são estatísticas.Parâmetro: Medida de uma característic da população (não é uma v..). Em geral, representada por \\(\\theta\\).Estatística: Característica associada aos dados de uma amostra. É uma função das variáveis aleatórias que constituem uma amostra. É portanto, uma variável aelatória.Estimador: É uma estatística construída com finalidade de representar, ou estimar, o parâmetro de interesse na população. estimativas são os valores numéricos assumidos pelos estimadores, usualmente denotados por \\(\\hat{\\theta}\\).","code":""},{"path":"teste-de-hipóteses.html","id":"análise-de-experimentos","chapter":"5 Teste de Hipóteses","heading":"5.2 Análise de Experimentos","text":"Em experimentos, nos referimos à investigações em que o pesquisador possuir controle sob condições de interesse, tornando viável inferência sobre relações de causa e efeito.\nPode-se definir experimento como um série de rodadas em que mudanças propositais são realizadas nas variáveis de input para poder verificar possíveis alterações na variávei resposta (Montgomery, 2013).Tratamentos: “intervenções” de interesse.Tratamentos: “intervenções” de interesse.Unidades experimentais: menor subdivisão experimento que o tratamento é atribuído.Unidades experimentais: menor subdivisão experimento que o tratamento é atribuído.Respostas: medida de interesse observada na unidade experimental.Respostas: medida de interesse observada na unidade experimental.Príncipios da Análise de Experimentos.\nRandomização\nReplicação\nBlocagem\nPríncipios da Análise de Experimentos.RandomizaçãoReplicaçãoBlocagemRandomização corresponde à aleatoriedade na alocação de tratamentos em unidades experimentaise nas rodadas experimento. Em geral, isso valida os pressupostos de independência de métodos estatísticos.Replicação corresponde à rodadas independentes em cada combinação de tratamentos/fatores. Permite obter uma estimativa erro experimental, consequentemente dos parâmetros (importante na comparação entre tratamentos).Blocagem é utilizado para reduzir variabilidade relativa à ruídos; fatores que podem influenciar resposta observada na unidade exeprimental, mas em que não temos interesse direto.","code":""},{"path":"teste-de-hipóteses.html","id":"experimentos-completamente-randomizados","chapter":"5 Teste de Hipóteses","heading":"5.3 Experimentos completamente randomizados","text":"idéia de experimentos completamente randomizados é que escolha tratamento ser atribuído para \\(\\)-ésima observação seja aleatória.Por exemplo, caso deseja-se realizar um experimento que compare duas marcas de fertilizantes “” e “B”, referente ao crescimento das plantas. Nesse caso, imagine que temos 10 plantas, dispostas em um mesmo terreno, plantadas em um mesmo determinado tempo, sob mesmas condições:Queremos então, que os tratamentos “” e “B” (referntes aos fertilizantes), sejam atribuídos aleatoriamente às plantas.Digamos então, que o experimento foi realizado, e os seguintes valores referentes ao crescimento da planta (em cm), foram observados:Observando o box-plot abaixo, o tratamento “” parece ter fornecido um maior impulsionamento crescimento das plantas, em relação ao tratamento “B”.Vamos osbervar então, como foi diferença das médias entre tratamentos:Porém, supondo não haver diferença entre os tratamentos, e que ambos tenham mesma distribuição, o quão “raro” seria observar exatamente essa diferença?O número de combinações possíveis é de \\({10\\choose 5} = 252\\)Dentre todas combinações possíveis, podemos analisar distirbuição de diferenças de médias, considerando os dados observados.partir daí, podemos ter ideia de que, dado suposição de que não há diferença entre médias, o resultado observado é um tanto quanto raro. Com esse conceito, temos então, indícios de que suposição de igualdade entre distribuições das médias não seja o que de fato ocorre.Assumindo independência entre observações, temos que observações possuem mesma probabilidade de receber certo tratamento. Então, probabilidade de observamos um valor maior ou tão extremo quanto o observado, é dada por:\\[ P( X \\ge x | H_0) = \\sum_{=1}^{N\\choose k} \\frac{(x_i \\le x*)}{N\\choose k}\\]exemplo, podemos calcular:E caso bilateral:E se ao invés de cinco replicações por tratamento, tivéssemos 10?E se fossem 20?O número de combinações torna-se muito grande \\({20\\choose 10} = 184756\\)Por outro lado, se tivéssemos apenas duas replicações, o número de combinações seria de apenas 6.Assim, replicação permite uma maior robustez quanto à estimativa da possível variabilidade presente fenômeno. Como em algum certo ponto é inviável simularmos todas possíveis ocorrências fenômeno, recorremos à distribuições de probabilidade que modelam o processo gerador de dados.Para comparar média dos dois tratamentos, podemos utilizar o teste-t para variâncias iguais.Que é equivalente ao resultado da anova, pois \\(t_v^2 = F_{1,v}\\).Mas o que representam esses resultados?Para avaliar diferença entre média de dois ou mais tratmentos, podemos utilizar análise de variância (ANOVA).","code":"\ntrat1 <- rep(c(\"A\", \"B\"), each = 5)\ntrat1 [1] \"A\" \"A\" \"A\" \"A\" \"A\" \"B\" \"B\" \"B\" \"B\" \"B\"\nset.seed(123)\nsample(trat1, 10) [1] \"A\" \"B\" \"A\" \"B\" \"B\" \"B\" \"A\" \"B\" \"A\" \"A\"\nset.seed(123)\ndf_planta <- tibble(planta = paste0(\"P\", 1:10),\n                    trat   = sample(trat1, 10))\n\ndf_planta# A tibble: 10 × 2\n   planta trat \n   <chr>  <chr>\n 1 P1     A    \n 2 P2     B    \n 3 P3     A    \n 4 P4     B    \n 5 P5     B    \n 6 P6     B    \n 7 P7     A    \n 8 P8     B    \n 9 P9     A    \n10 P10    A    \ndf_planta <- df_planta %>%\n  mutate(y_cm = c(14.22, 5.59, 15.41, 10.34, 9.73,\n                  12.32, 16.15, 10.80, 15.29, 12.44 ) )\n\ndf_planta# A tibble: 10 × 3\n   planta trat   y_cm\n   <chr>  <chr> <dbl>\n 1 P1     A     14.2 \n 2 P2     B      5.59\n 3 P3     A     15.4 \n 4 P4     B     10.3 \n 5 P5     B      9.73\n 6 P6     B     12.3 \n 7 P7     A     16.2 \n 8 P8     B     10.8 \n 9 P9     A     15.3 \n10 P10    A     12.4 \ntapply(df_planta$y_cm, df_planta$trat, mean)     A      B \n14.702  9.756 \nggplot(df_planta, \n       aes(x = trat, y = y_cm, fill = trat)) +\n  geom_boxplot()\n  df_planta %>%\n    group_by(trat) %>%\n    summarise(media = mean(y_cm)) %>%\n    .$media %>%\n    dist()      1\n2 4.946\nchoose(10, 5)[1] 252\ny1 <- df_planta$y_cm\n\nmedias <- combn(y1, m = 5)\n\nN = choose(10, 5)\ndif_media <- rep(NA, N)\n\n  for(i in 1:N) {\n    dif_media[i] <- mean(medias[,i]) - mean(setdiff(y1, medias[,i]))   \n  }\np1 <- ggplot() +\n    geom_histogram(aes(dif_media),\n                   fill = \"cyan4\",\n                   col = \"white\",\n                   bins = 30) +\n    geom_vline(xintercept = c(quantile(dif_media, 0.025),\n                              quantile(dif_media, 0.975)),\n               col = \"firebrick\",\n               linetype = \"dashed\") +\n    geom_point(x = 4.946, y = 0, size = 1.5, col = \"purple\") +\n    theme_minimal()\np1\nsum(dif_media >= 4.946) / N[1] 0.003968254\n2*sum(dif_media >= 4.946) / N[1] 0.007936508Error: não é possível alocar vetor de tamanho 63.2 Gb\nt.test(df_planta$y_cm[df_planta$trat == \"A\"],\n       df_planta$y_cm[df_planta$trat == \"B\"],\n       var.equal = TRUE)\n    Two Sample t-test\n\ndata:  df_planta$y_cm[df_planta$trat == \"A\"] and df_planta$y_cm[df_planta$trat == \"B\"]\nt = 3.8129, df = 8, p-value = 0.005142\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 1.954676 7.937324\nsample estimates:\nmean of x mean of y \n   14.702     9.756 \naov(y_cm ~ trat, data = df_planta) %>%\n  summary()            Df Sum Sq Mean Sq F value  Pr(>F)   \ntrat         1  61.16   61.16   14.54 0.00514 **\nResiduals    8  33.65    4.21                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n3.8129^2[1] 14.53821"},{"path":"teste-de-hipóteses.html","id":"análise-de-variância-para-um-fator","chapter":"5 Teste de Hipóteses","heading":"5.4 Análise de Variância para um fator","text":"Análise de variância (ANOVA) é um modelo linear, pois variável resposta é modelada como uma função linear dos parâmetros. Utilizado para análise entre grupos e dentre grupos, o modelo realiza contrastes entre classes, resultando na comparação entre médias de tratamentos.\\[\n\\begin{aligned}\n\\text{H}_0 &: \\ \\mu_1 = \\mu_2 = \\cdots = \\mu_k \\\\\n\\text{H}_1 &: \\ \\mu_i \\ne \\mu_j \\ \\text{para algum par}\\  \\ ,j \n\\end{aligned}\n\\]Pressupostos\nAmostras independentes\nNormalidade dos resíduos\nHomocedasticidade\nSensível outliers\nAmostras independentesNormalidade dos resíduosHomocedasticidadeSensível outliers“O interrogatório habilidoso da natureza”: O experimento de Fisher\"\nTeste F\nComparaçã de médias: Teste de Tukey\nTeste FComparaçã de médias: Teste de TukeyUm conjunto de procedimentos que têm como base comparação entre variâncias com o propósito de verificar em que medida diferenças nas médias de respostas de grupos diferentes são estatisticamente significativas (Tabachnick & Fidell, 2019).especificação modelo é dada por:\\[y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\\]\nem que \\(\\mu\\) representa média geral, \\(\\tau_i\\) representa o \\(\\)-ésimo efeito de tratamento e \\(\\epsilon_{ij}\\) representa o erro aleatório.","code":"\nsimula <- function(mu, t1, t2, sigma, n) {\n  \n  y1 <- mu + t1 + rnorm(n, 0, sigma)\n  y2 <- mu + t2 + rnorm(n, 0, sigma)\n  y3 <- mu + rnorm(n, 0, sigma)\n  \n  yy <- c(y1, y2, y3)\n  \n  trat <- as.factor( c(rep(LETTERS[1], n), rep(LETTERS[2], n), rep(LETTERS[3], n)) )\n  \n  df <- data.frame(y = yy,\n                   trat = trat)\n  \n  return(list(dados = df))\n  \n}\nset.seed(123)\nsim_df <- simula(mu = 10, t1 = 5, t2 = -5, sigma = 5, n = 10)\ncar::leveneTest(y ~ trat, data = sim_df$dados)Levene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(>F)\ngroup  2   5e-04 0.9995\n      27               \nmod <- aov(y ~ trat, data = sim_df$dados)\nsummary(mod)            Df Sum Sq Mean Sq F value   Pr(>F)    \ntrat         2  488.7  244.34   10.27 0.000481 ***\nResiduals   27  642.1   23.78                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nshapiro.test(mod$residuals)## \n##  Shapiro-Wilk normality test\n## \n## data:  mod$residuals\n## W = 0.96213, p-value = 0.3507\nTukeyHSD(mod)  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = y ~ trat, data = sim_df$dados)\n\n$trat\n         diff        lwr       upr     p adj\nB-A -9.330018 -14.737296 -3.922740 0.0006008\nC-A -7.495923 -12.903201 -2.088645 0.0052775\nC-B  1.834096  -3.573182  7.241374 0.6812485\nplot(TukeyHSD(mod, conf.level = 0.95),las=1, col = \"red\")"},{"path":"teste-de-hipóteses.html","id":"experimentos-randomizados-blocados","chapter":"5 Teste de Hipóteses","heading":"5.5 Experimentos Randomizados Blocados","text":"Experimentos com bloco ocorrem quando há um fator conhecido e controlável, que pode ser responsável por algum tipo de intervenção na resposta.Imagine exemplo anterior sobre plantas. Mas temos 3 fertilizantes, e metade das plantas estão em uma estufa e outra metade céu aberto. Talvez, esse fator possa exercer alguma influência nas médias observadas. Assim, adicionamos um fator de controle.\\[y_{ijk} = \\mu + \\beta_j + \\tau_i + \\epsilon_{ijk}\\]em que \\(\\beta_j\\) representa área (estufa ou céu aberto). Assim, teríamos três tratamentos e dois blocos.","code":"\nmod2 <- aov(y ~ bloco + trat, data = df_planta2)\ndata.frame(y = df_planta2$y, model.matrix(mod2) ) %>%\n  slice(c(1:5, 20:25, 40:45))           y X.Intercept. blocoE tratB tratC\n1  11.796529            1      0     0     0\n2  14.346075            1      0     0     0\n3  11.921987            1      0     0     0\n4  12.813326            1      0     0     0\n5  13.124882            1      0     0     0\n20 13.858587            1      0     0     0\n21  7.915879            1      0     1     0\n22  9.376248            1      0     1     0\n23  6.203811            1      0     1     0\n24 16.506868            1      0     1     0\n25 13.623886            1      0     1     0\n40 10.647825            1      1     1     0\n41  6.279393            1      1     0     1\n42  4.114786            1      1     0     1\n43  7.685377            1      1     0     1\n44  7.634400            1      1     0     1\n45  7.464743            1      1     0     1"},{"path":"teste-de-hipóteses.html","id":"experimentos-fatoriais","chapter":"5 Teste de Hipóteses","heading":"5.6 Experimentos Fatoriais","text":"Um experimento completamente randomizado com dois fatores, pode ser representado por:\\[y_{ijk} = \\mu_{ij} + \\epsilon_{ijk}\\]\nem que \\(\\) e \\(j\\) representam os níveis primeiro e segundo fator, respectivamente, e \\(k\\) denota \\(k\\)-ésima replicação. O modelo também pode ser escrito utilizando representação dos efeitos, como:\\[y_{ijk} = \\mu + \\alpha_i + \\tau_j + \\alpha\\tau_{ij} + \\epsilon_{ijk}\\]\nem que \\(alpha_i\\) e \\(beta_j\\) representam diferença marginal da média total entre os experimentos, \\(\\)-ésimo nível primeiro fator e \\(j\\)-ésimo nível segundo fator. interação \\(\\alpha\\beta_{ij}\\) representa diferença entre o valor principal e os efeitos marginais isolados.","code":""},{"path":"teste-de-hipóteses.html","id":"outros-modelos","chapter":"5 Teste de Hipóteses","heading":"5.7 Outros Modelos","text":"Repostas não-gaussianas.Repostas não-gaussianas.Quadrados Latinos.Quadrados Latinos.Confundimento.Confundimento.Medidas repetidas.Medidas repetidas.","code":""},{"path":"otimização.html","id":"otimização","chapter":"6 Otimização","heading":"6 Otimização","text":"","code":""},{"path":"otimização.html","id":"estatística-e-machine-learning","chapter":"6 Otimização","heading":"6.1 Estatística e Machine Learning","text":"abordagem estatística para análise e resumo de informações contidas em um conjunto de dados, consiste na suposição de que existe um mecanismo estocástico gerador processo em análise (Bonat et al. 2012). Para isso, existem técnica que nos auxiliam na estimação de uma função (e.g. de regressão) que são de fundamental importância em estatística e machine learning.Muitas ténicas de regressão usadas hoje são datadas de muitos anos atrás. Todavia, com o avanço computacional, métodos mais robustos em relação ao real processo gerador vêm ganhando cada vez mais espaço e importância(Izbicki Santos 2020). Por exemplo, em modelos com mais covariáveis que observações, métodos tradicionais sofrem pela falta de graus de liberdade.De maneira geral, nosso objetivo é determinar uma relação entre uma variável aleatória de interesse \\(Y\\) e um vetor de covariáveis \\(x = (x_1, ..., x_n)\\). Então, temos que:\\[ g(x) := E [ Y | X = x]\\]","code":""},{"path":"otimização.html","id":"regressão-linear","chapter":"6 Otimização","heading":"6.2 Regressão Linear","text":"Regressão Linear configura um dos algoritmos mais simples em aprendizado supervisionado, porém, é um modelo de extrema importância. Além de ser bastante eficaz em diversas aplicações, seu bom entendimento é resposável por fundamentos sólidos em conceitos de modelagem, pois diversas modelagens mais sofisticadas podem ser vistas como generalizações de uma regressão linear. Esta seção apresenta alguns conceitos de regressão linear. Um estudo extremamente rico compreensivo assunto pode ser visto em Kutner et al. (2005).Em um modelo de regressão linear simples, temos seguinte relação:\\[\\begin{align}\n  y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\qquad  \\qquad = 1, ..., n \\tag{6.1}\n\\end{align}\\]em que \\(y_i\\) é variávei de interesse (dependente), \\(x_i\\) é variável explicativa (independente), \\(\\beta_0\\) e \\(\\beta_1\\) são os parâmetros serem estimados pelo modelo e \\(\\epsilon_i\\) representa o erro associdao à -ésima observação.Vamos considerar um exemplo utilizando os dados mtcars (disponível em R), que consiste em informações sobre automóveis.Quanto mais pesado o carro, menor autonomia (quantidade de milhas percorridas por galão)?\\[Autonomia_i = \\beta_0 + \\beta_1 Peso_i + \\epsilon_i\\]Para encontrar reta que melhor se ajusta aos pontos utilizamos abordagem de mínimos quadrados, que consiste em encontrar estimadores \\(\\hat{\\beta_0}\\) e \\(\\hat{\\beta_1}\\) que minimizam soma dos erros quadráticos.\\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\]\n\\[\\epsilon_i = y_i - \\beta_0 - \\beta_1 x_i\\]\n\\[\\epsilon_i^2 = (y_i - \\beta_0 - \\beta_1 x_i)^2\\]\n\\[\\sum_{= 1}^{n} \\epsilon_i^{2} = \\sum_{=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2\\]Temos então, nossa função de custo, que queremos minimzar:\\[\\begin{align}\n  \\sum_{= 1}^{n} \\epsilon_i^{2} = \\sum_{=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2 \\tag{6.2}\n\\end{align}\\]","code":""},{"path":"otimização.html","id":"regressão-linear-simples","chapter":"6 Otimização","heading":"6.2.1 Regressão Linear Simples","text":"Casos em que temos apenas uma covariável (x) como variável explicativa (como o exemplo acima), dizemos estar em uma regressão linear simples.Para minimizar função, aplicamos derivadas parciais, em respeito \\(\\hat{\\beta_0}\\) e \\(\\hat{\\beta_1}\\), e igualamos zero.\nComo minimizar uma função?Para minimizar uma função \\(f(x)\\), calculamos sua derivada e igualamos zero. Para verificar que \\(f'(x) = 0\\) é um ponto de mínimo, devemos observar o sinal da segunda derivada, \\(f''(x)\\).Fazendo derivadas parciais da função de custo (L) em relação \\(\\beta_0\\) e em relação \\(\\beta_1\\), temos:\\[L = \\sum_{=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2\\]\n\\[\\begin{align}\n\\frac{\\partial L}{\\partial \\beta_0} = -2 \\sum_{=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i) \\tag{6.3}\n\\end{align}\\]\\[\\begin{align}\n\\frac{\\partial L}{\\partial \\beta_1} = -2 \\sum_{=1}^{n} x_i(y_i - \\beta_0 - \\beta_1 x_i) \\tag{6.4}\n\\end{align}\\]Igualando \\(\\frac{\\partial L}{\\partial \\beta_0} = 0\\) e \\(\\frac{\\partial L}{\\partial \\beta_1} = 0\\), obtemos equações:\\[\\begin{align}\n  \\sum_{=1}^{n} (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0  \\rightarrow n \\hat{\\beta}_0 + \\hat{\\beta}_1 \\sum_{=1}^{n} x_i = \\sum_{=1}^{n} y_i \\tag{6.5}\n\\end{align}\\]\\[\\begin{align}\n  \\sum_{=1}^{n} x_i(y_i - \\beta_0 - \\beta_1 x_i) = 0   \\rightarrow \\hat{\\beta}_0 \\sum_{=1}^{n} x_i + \\hat{\\beta}_1 \\sum_{=1}^{n} x_i^2 = \\sum_{=1}^{n} x_i y_i \\tag{6.6}\n\\end{align}\\]De (6.5) temos então:\\[\\begin{align}\n\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}\n\\tag{6.7}\n\\end{align}\\]Substituindo (6.7) em (6.6), temos:\\[\n\\hat{\\beta_1}  = \\frac{\\sum_{=1}^{n} x_i y_i - \\frac{1}{n}\\sum_{=1}^{n}x_i \\sum_{=1}^{n} y_i  }{\\sum_{=1}^{n}x^2 - \\frac{1}{n} (\\sum_{=1}^{n}x_i)^2} = \n\\frac{n \\sum_{=1}^{n} x_i y_i - \\sum_{=1}^{n} x_i \\sum_{=1}^{n} y_i}{n \\sum_{=1}^{n} x_i^2 - (\\sum_{=1}^{n} x_i)^2}\n\\]\\[\\begin{align}\n\\hat{\\beta_1} = \\frac{\\sum_{=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{=1}^{n}(x_i - \\bar{x})^2} \\tag{6.8}\n\\end{align}\\]Exercício. Fazer “na mão”, comparando com o resultado da função lm().","code":""},{"path":"otimização.html","id":"forma-matricial","chapter":"6 Otimização","heading":"6.2.2 Forma Matricial","text":"O modelo visto acima pode ser representado matricialmente, generalizando para o caso em que temos mais de uma variável explicativa.\\[y_1 = \\beta_0 + \\beta_1 x_1 + \\epsilon_1\\]\n\\[y_2 = \\beta_0 + \\beta_1 x_2 + \\epsilon_2\\]\n\\[\\vdots\\]\n\\[y_n = \\beta_0 + \\beta_1 x_n + \\epsilon_n\\]\\[\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n  \\end{bmatrix} = \\begin{bmatrix} 1 & x_{11}\\\\ 1 & x_{12} \\\\ \\vdots & \\vdots \\\\ 1 & x_{1n}  \\end{bmatrix} \\times \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\  \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n  \\end{bmatrix}\\]\\[\\Large \\boldsymbol{Y} = \\boldsymbol{XB} + \\boldsymbol{\\epsilon}\\]Como estimar os parâmetros com representação matricial? Mesma idéia: minimizar os erros!\\[\\boldsymbol{Y} = \\boldsymbol{X\\beta} + \\boldsymbol{\\epsilon}\\]\n\\[\\boldsymbol{\\epsilon} = \\boldsymbol{Y} - \\boldsymbol{X\\beta}\\]Mínimos quadrados: como representar soma quadrática dos erros matricialmente?\n\\[\\boldsymbol{\\epsilon}' \\boldsymbol{\\epsilon}\\]\nEntão,\\[\\boldsymbol{\\epsilon}' \\boldsymbol{\\epsilon} = (\\boldsymbol{Y} - \\boldsymbol{X\\beta})'(\\boldsymbol{Y} - \\boldsymbol{X\\beta})\\]Novamente: mesma ideia, derivar e igualar zero.\\[\\boldsymbol{Y} = \\boldsymbol{X\\beta} + \\boldsymbol{\\epsilon}\\]\n\\[\\boldsymbol{\\epsilon} = \\boldsymbol{Y} - \\boldsymbol{X\\beta}\\]\n\\[\\boldsymbol{\\epsilon}' \\boldsymbol{\\epsilon} = (\\boldsymbol{Y} - \\boldsymbol{X\\beta})'(\\boldsymbol{Y} - \\boldsymbol{X\\beta})\\]\n\\[= (\\boldsymbol{Y} - \\boldsymbol{X\\beta})'(\\boldsymbol{Y} - \\boldsymbol{X\\beta})\\]\n\\[= (\\boldsymbol{Y'} - \\boldsymbol{\\beta'X'})(\\boldsymbol{Y} - \\boldsymbol{X\\beta})\\]\n\\[= \\boldsymbol{Y'}Y - \\boldsymbol{\\beta'X'Y} - \\boldsymbol{Y'X\\beta} + \\boldsymbol{\\beta'X'X\\beta}\\]\n\\[= \\boldsymbol{Y'Y} - 2\\boldsymbol{\\beta'X'Y}  + \\boldsymbol{\\beta'X'X\\beta}\\]\n\\[-2\\boldsymbol{X'Y}  + 2\\boldsymbol{X'X\\beta} = 0\\]\n\\[\\boldsymbol{X'X\\beta} = \\boldsymbol{X'Y}\\]\n\\[\\boldsymbol{\\hat{\\beta}} = \\boldsymbol{(X'X)^{-1}X'Y}\\]","code":""},{"path":"otimização.html","id":"decomposição-qr","chapter":"6 Otimização","heading":"6.2.3 Decomposição QR","text":"Operações envolvendo inversa de uma matriz podem ser bastante custosas ponto de vista computacional, principalmente quando matriz cresce em dimensões. Uma alternativa bastante utilizada é decomposição \\(QR\\) (através algoritmo Gram-Schmidt, por exemplo).Sendo \\(X_{n\\times p}\\) uma matriz de posto completo (\\(n\\) > \\(p\\), rank=\\(p\\)), então podemos decompor \\(X\\) em \\[X = QR\\]em que:\\(Q\\) tem dimensões \\(n\\times p\\) e \\(Q'Q = \\)\\(R\\) tem dimensões \\(p \\times p\\) e é uma matriz triangular superiorDessa maneira, podemos reescrever o processo de mínimos quadrados acima, como:\\[\\begin{align}\n&\\boldsymbol{X'X\\beta} &\\ = \\quad &\\boldsymbol{X'Y} \\\\\n&\\boldsymbol{(QR)'(QR)\\beta} &\\ = \\quad  &\\boldsymbol{(QR)'Y} \\\\\n&\\boldsymbol{R'R\\beta} &\\ = \\quad  &\\boldsymbol{R'Q'Y} \\\\\n&\\boldsymbol{(R')^{-1}R'R\\beta} &\\ = \\quad & \\boldsymbol{(R')^{-1}R'Q'Y} \\\\\n&\\boldsymbol{R\\beta} &\\ = \\quad & \\boldsymbol{Q'Y} \\\\\n\\end{align}\\]\\[\\boldsymbol{R\\beta} = \\boldsymbol{Q'Y}\\]Dessa maneira, podemos chegar nos valores preditos \\(\\hat{y}\\) através de \\(\\boldsymbol{\\hat{Y}} = \\boldsymbol{QRR^{-1}Q'} = \\boldsymbol{QQ'Y}\\)Exercício: adicione colinearidade na regressão dos dados mtcars e compare o resultado da função lm() R e sklearn.linear_model.LinearRegression() python.","code":""},{"path":"otimização.html","id":"gradiente-descendente","chapter":"6 Otimização","heading":"6.3 Gradiente Descendente","text":"Gradiente descendente . . .\nFigure 6.1: Gradiente descendente minimiza função de custo gradualmente, ajustando o(s) parâmetro(s) em cada passo (figura adaptada de Boehmke & Greenwell, 2019).\n","code":""},{"path":"other-considerations.html","id":"other-considerations","chapter":"7 Other Considerations","heading":"7 Other Considerations","text":"like tell something.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"Bonat, Wagner Hugo, Paulo Justiniano Ribeiro Jr, Elias Teixeira Krainski, Walmes Marques Zeviani. 2012. “Métodos Computacionais Em Inferência Estatı́stica.” SINAPE. Curitiba: Associação Brasileira de Estatı́stica-ABE.Izbicki, Rafael, Tiago Mendonça dos Santos. 2020. Aprendizado de Máquina: Uma Abordagem Estatı́stica. Rafael Izbicki.Kutner, Michael H, Christopher J Nachtsheim, John Neter, William Li, others. 2005. Applied Linear Statistical Models. Vol. 5. McGraw-Hill Irwin Boston.Xie, Yihui. 2015. Dynamic Documents R Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://yihui.name/knitr/.———. 2023. Bookdown: Authoring Books Technical Documents R Markdown. https://CRAN.R-project.org/package=bookdown.","code":""}]
