# Otimização

## Estatística e Machine Learning

A abordagem estatística para análise e resumo de informações contidas em um conjunto de dados, consiste na suposição de que existe um mecanismo estocástico gerador do processo em análise [@bonat2012metodos]. Para isso, existem técnica que nos auxiliam na estimação de uma função (e.g. de regressão) que são de fundamental importância em estatística e machine learning.

Muitas ténicas de regressão usadas hoje são datadas de muitos anos atrás. Todavia, com o avanço computacional, métodos mais robustos em relação ao real processo gerador vêm ganhando cada vez mais espaço e importância[@izbicki2020aprendizado]. Por exemplo, em modelos com mais covariáveis que observações, métodos tradicionais sofrem pela falta de graus de liberdade.

De maneira geral, nosso objetivo é determinar uma relação entre uma variável aleatória de interesse $Y$ e um vetor de covariáveis $x = (x_1, ..., x_n)$. Então, temos que:

$$ g(x) := E [ Y | X = x]$$ 


## Regressão Linear

_Regressão Linear_ configura um dos algoritmos mais simples em aprendizado supervisionado, porém, é um modelo de extrema importância. Além de ser bastante eficaz em diversas aplicações, seu bom entendimento é resposável por fundamentos sólidos em conceitos de modelagem, pois diversas modelagens mais sofisticadas podem ser vistas como generalizações de uma regressão linear. Esta seção apresenta alguns conceitos de regressão linear. Um estudo extremamente rico compreensivo no assunto pode ser visto em @kutner2005applied.

Em um modelo de regressão linear simples, temos a seguinte relação:

\begin{align}
  y_i = \beta_0 + \beta_1 x_i + \epsilon_i \qquad  \qquad i = 1, ..., n (\#eq:um)
\end{align}

em que $y_i$ é a variávei de interesse (dependente), $x_i$ é a variável explicativa (independente), $\beta_0$ e $\beta_1$ são os parâmetros a serem estimados pelo modelo e $\epsilon_i$ representa o erro associdao à i-ésima observação.

Vamos considerar um exemplo utilizando os dados `mtcars` (disponível em `R`), que consiste em informações sobre automóveis.
```{r, echo=FALSE, message=FALSE, warning=FALSE}
data(mtcars)
knitr::kable(head(mtcars, 3))
```
Quanto mais pesado o carro, menor a autonomia (quantidade de milhas percorridas por galão)?

$$Autonomia_i = \beta_0 + \beta_1 Peso_i + \epsilon_i$$

```{r plot-last2, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)

mtcars$pred <- lm(mpg ~ wt, data = mtcars)$fitted.values

ggplot(data = mtcars, aes(x = wt, y = mpg)) +
  labs(x = "Peso", y = "Milhas p/ galão") +
  geom_segment(aes(xend = wt, yend = pred), alpha = .4, col = "firebrick") +  # alpha 
  geom_smooth(method="lm", se = FALSE) +
  geom_point(size = 2.5, col = "cyan4") +
  #geom_point(aes(y = pred), shape = 1) +
  ylim(c(8,34)) +
  theme_minimal()
```

Para encontrar a reta que melhor se ajusta aos pontos utilizamos a abordagem de mínimos quadrados, que consiste em encontrar estimadores $\hat{\beta_0}$ e $\hat{\beta_1}$ que minimizam a soma dos erros quadráticos. 

$$y_i = \beta_0 + \beta_1 x_i + \epsilon_i$$
$$\epsilon_i = y_i - \beta_0 - \beta_1 x_i$$
$$\epsilon_i^2 = (y_i - \beta_0 - \beta_1 x_i)^2$$
$$\sum_{i = 1}^{n} \epsilon_i^{2} = \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2$$

Temos então, nossa função de custo, que queremos minimzar:

\begin{align}
  \sum_{i = 1}^{n} \epsilon_i^{2} = \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2 (\#eq:dois)
\end{align}

### Regressão Linear Simples

Casos em que temos apenas uma covariável (x) como variável explicativa (como o exemplo acima), dizemos estar em uma regressão linear simples.

Para minimizar a função, aplicamos derivadas parciais, em respeito a $\hat{\beta_0}$ e $\hat{\beta_1}$, e igualamos a zero.
Como minimizar uma função?

Para minimizar uma função $f(x)$, calculamos sua derivada e igualamos a zero. Para verificar que $f'(x) = 0$ é um ponto de mínimo, devemos observar o sinal da segunda derivada, $f''(x)$.

```{r, warning=FALSE, message=FALSE, echo=FALSE, out.width="40%", fig.align = 'center', fig.cap=""}
knitr::include_graphics('images/simple_tangent_graph.png')
```

Fazendo as derivadas parciais da função de custo (L) em relação a $\beta_0$ e em relação a $\beta_1$, temos:

$$L = \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2$$
\begin{align}
\frac{\partial L}{\partial \beta_0} = -2 \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i) (\#eq:tres)
\end{align}

\begin{align}
\frac{\partial L}{\partial \beta_1} = -2 \sum_{i=1}^{n} x_i(y_i - \beta_0 - \beta_1 x_i) (\#eq:cinco)
\end{align}


Igualando $\frac{\partial L}{\partial \beta_0} = 0$ e $\frac{\partial L}{\partial \beta_1} = 0$, obtemos as equações:

\begin{align}
  \sum_{i=1}^{n} (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0	\rightarrow n \hat{\beta}_0 + \hat{\beta}_1 \sum_{i=1}^{n} x_i = \sum_{i=1}^{n} y_i (\#eq:seis)
\end{align}

\begin{align}
  \sum_{i=1}^{n} x_i(y_i - \beta_0 - \beta_1 x_i) = 0	\rightarrow \hat{\beta}_0 \sum_{i=1}^{n} x_i + \hat{\beta}_1 \sum_{i=1}^{n} x_i^2 = \sum_{i=1}^{n} x_i y_i (\#eq:sete)
\end{align}

De \@ref(eq:seis) temos então:

\begin{align}
\hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x}
(\#eq:b0)
\end{align}

Substituindo \@ref(eq:b0) em \@ref(eq:sete), temos:

$$
\hat{\beta_1}  = \frac{\sum_{i=1}^{n} x_i y_i - \frac{1}{n}\sum_{i=1}^{n}x_i \sum_{i=1}^{n} y_i  }{\sum_{i=1}^{n}x^2 - \frac{1}{n} (\sum_{i=1}^{n}x_i)^2} = 
\frac{n \sum_{i=1}^{n} x_i y_i - \sum_{i=1}^{n} x_i \sum_{i=1}^{n} y_i}{n \sum_{i=1}^{n} x_i^2 - (\sum_{i=1}^{n} x_i)^2}
$$

\begin{align}
\hat{\beta_1} = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^2} (\#eq:b1)
\end{align}


---
Exercício. Fazer "na mão", comparando com o resultado da função lm().

### Forma Matricial

O modelo visto acima pode ser representado matricialmente, generalizando para o caso em que temos mais de uma variável explicativa. 

$$y_1 = \beta_0 + \beta_1 x_1 + \epsilon_1$$
$$y_2 = \beta_0 + \beta_1 x_2 + \epsilon_2$$
$$\vdots$$
$$y_n = \beta_0 + \beta_1 x_n + \epsilon_n$$

$$\begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n  \end{bmatrix} = \begin{bmatrix} 1 & x_{11}\\ 1 & x_{12} \\ \vdots & \vdots \\ 1 & x_{1n}  \end{bmatrix} \times \begin{bmatrix} \beta_0 \\ \beta_1 \\  \end{bmatrix} + \begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n  \end{bmatrix}$$

$$\Large \boldsymbol{Y} = \boldsymbol{XB} + \boldsymbol{\epsilon}$$

Como estimar os parâmetros com a representação matricial? Mesma idéia: minimizar os erros!

$$\boldsymbol{Y} = \boldsymbol{X\beta} + \boldsymbol{\epsilon}$$
$$\boldsymbol{\epsilon} = \boldsymbol{Y} - \boldsymbol{X\beta}$$

Mínimos quadrados: como representar soma quadrática dos erros matricialmente?
$$\boldsymbol{\epsilon}' \boldsymbol{\epsilon}$$
Então,

$$\boldsymbol{\epsilon}' \boldsymbol{\epsilon} = (\boldsymbol{Y} - \boldsymbol{X\beta})'(\boldsymbol{Y} - \boldsymbol{X\beta})$$

Novamente: mesma ideia, derivar e igualar a zero.

$$\boldsymbol{Y} = \boldsymbol{X\beta} + \boldsymbol{\epsilon}$$
$$\boldsymbol{\epsilon} = \boldsymbol{Y} - \boldsymbol{X\beta}$$
$$\boldsymbol{\epsilon}' \boldsymbol{\epsilon} = (\boldsymbol{Y} - \boldsymbol{X\beta})'(\boldsymbol{Y} - \boldsymbol{X\beta})$$
$$= (\boldsymbol{Y} - \boldsymbol{X\beta})'(\boldsymbol{Y} - \boldsymbol{X\beta})$$
$$= (\boldsymbol{Y'} - \boldsymbol{\beta'X'})(\boldsymbol{Y} - \boldsymbol{X\beta})$$
$$= \boldsymbol{Y'}Y - \boldsymbol{\beta'X'Y} - \boldsymbol{Y'X\beta} + \boldsymbol{\beta'X'X\beta}$$
$$= \boldsymbol{Y'Y} - 2\boldsymbol{\beta'X'Y}  + \boldsymbol{\beta'X'X\beta}$$
$$-2\boldsymbol{X'Y}  + 2\boldsymbol{X'X\beta} = 0$$
$$\boldsymbol{X'X\beta} = \boldsymbol{X'Y}$$
$$\boldsymbol{\hat{\beta}} = \boldsymbol{(X'X)^{-1}X'Y}$$

### Decomposição QR

Operações envolvendo a inversa de uma matriz podem ser bastante custosas do ponto de vista computacional, principalmente quando a matriz cresce em dimensões. Uma alternativa bastante utilizada é a decomposição $QR$ (através do algoritmo Gram-Schmidt, por exemplo).

Sendo $X_{n\times p}$ uma matriz de posto completo ($n$ > $p$, rank=$p$), então podemos decompor $\boldsymbol{X}$ em $$\boldsymbol{X} = \boldsymbol{QR}$$

em que:

- $\boldsymbol{Q}$ tem dimensões $n\times p$ e $\boldsymbol{Q'Q} = \boldsymbol{I}$
- $\boldsymbol{R}$ tem dimensões $p \times p$ e é uma matriz triangular superior 

Dessa maneira, podemos reescrever o processo de mínimos quadrados acima, como:



\begin{align}
&\boldsymbol{X'X\beta} &\ = \quad &\boldsymbol{X'Y} \\
&\boldsymbol{(QR)'(QR)\beta} &\ = \quad  &\boldsymbol{(QR)'Y} \\
&\boldsymbol{R'R\beta} &\ = \quad  &\boldsymbol{R'Q'Y} \\
&\boldsymbol{(R')^{-1}R'R\beta} &\ = \quad & \boldsymbol{(R')^{-1}R'Q'Y} \\
&\boldsymbol{R\beta} &\ = \quad & \boldsymbol{Q'Y} \\
\end{align}

$$\boldsymbol{R\beta} = \boldsymbol{Q'Y}$$

Dessa maneira, podemos chegar nos valores preditos  $\hat{y}$ através de $\boldsymbol{\hat{Y}} = \boldsymbol{QRR^{-1}Q'} = \boldsymbol{QQ'Y}$

> **_Exercício_**: adicione colinearidade na regressão dos dados mtcars e compare o resultado da função `lm()` no R e `sklearn.linear_model.LinearRegression()` do python.

## Gradiente Descendente


Gradiente descendente é um dos algoritmos de otimização mais populares e amplamente utilizado, principalmente em otimização de redes neurais. O algoritmo busca encontrar o mínimo de uma função $J(\beta)$ de forma iterativa, em que $\beta$ são os parâmetros do modelo. A ideia consiste em atualizar os parâmetros na direção oposta do gradiente $\nabla_{\beta} J(\beta)$ em relação aos parâmetros [@ruder2016overview], ou seja, seguimos a direção da inclinação da região de resposta criada, em busca de um vale. Em sua forma mais simples, o algoritmo é definido como:

\begin{align}
\beta^{(k+1)} = \beta^{(k)} - \alpha_k \nabla J(\beta^{k}), \qquad k = 0, 1, \cdots
(\#eq:gradientedesc)
\end{align}

Em que $\beta^{(k)}$ são os valores do vetor de parâmetros no passo $k$, $\alpha$ é a taxa de aprendizado (_learning rate_) e $\nabla_{\beta} J(\beta)$ é o gradiente da função de custo $L(\cdot)$, ou seja, o vetor de derivadas parciais em relação aos parâmetros $\beta$. A taxa de aprendizado $\alpha$ determina o tamanho do passo em cada atualização, visando atingir o mínimo (local).

```{r gradient-descent-fig, warning=FALSE, message=FALSE, echo=FALSE, fig.height=4, fig.width=7, fig.align = 'center', fig.cap="Gradiente descendente minimiza a função de custo gradualmente, ajustando o(s) parâmetro(s) em cada passo (figura adaptada de Boehmke & Greenwell, 2019)."}

# create data to plot
x <- seq(-5, 5, by = .05)
y <- x^2 + 3
df <- data.frame(x, y)
step <- 5
step_size <- .2
for(i in seq_len(18)) {
  next_step <- max(step) + round(diff(range(max(step), which.min(df$y))) * step_size, 0)
  step <- c(step, next_step)
  next
}
steps <- df[step, ] %>%
  mutate(x2 = lag(x), y2 = lag(y)) %>%
  dplyr::slice(1:18)
# plot
ggplot(df, aes(x, y)) +
  geom_line(size = 1.5, alpha = .5) +
  theme_classic() +
  scale_y_continuous("Função de perda", limits = c(0, 30)) +
  xlab(expression(beta)) +
  geom_segment(data = df[c(5, which.min(df$y)), ], aes(x = x, y = y, xend = x, yend = -Inf), lty = "dashed") +
  geom_point(data = filter(df, y == min(y)), aes(x, y), size = 4, shape = 21, fill = "yellow") +
  geom_point(data = steps, aes(x, y), size = 3, shape = 21, fill = "blue", alpha = .5) +
  geom_curve(data = steps, aes(x = x, y = y, xend = x2, yend = y2), curvature = 1, lty = "dotted") +
  theme(
    text = element_text(size=15),
    axis.ticks = element_blank(),
    axis.text = element_blank()
  ) +
  annotate("text", x = df[5, "x"], y = 1, label = "Valor inicial", hjust = -0.1, vjust = .8, size=5) +
  annotate("text", x = df[which.min(df$y), "x"], y = 1, label = "Mínimo", hjust = -0.1, vjust = .8, size=5) +
  annotate("text", x = df[1, "x"]-0.75, y = df[10, "y"], label = "Taxa de aprendizado", hjust = -.8, vjust = 0, size=5)
```

Com uma taxa de aprendizado muito pequena são necessários muitas iterações para encontrar o mínimo. Todavia, caso a taxa de aprendizado seja muito grande, você pode acabar "pulando" o ponto de mínimo desejado.

```{r gradient-descent-step, warning=FALSE, message=FALSE, echo=FALSE, fig.height=3, fig.width=7, fig.align = 'center', fig.cap="Taxa de aprendizado pequena e grande (figura adaptada de Boehmke & Greenwell, 2019)."}
#library(ggplot2)
# create too small of a learning rate
step <- 5
step_size <- .05
for(i in seq_len(10)) {
  next_step <- max(step) + round(diff(range(max(step), which.min(df$y))) * step_size, 0)
  step <- c(step, next_step)
  next
}
too_small <- df[step, ] %>%
  mutate(x2 = lag(x), y2 = lag(y))
# plot
p1 <- ggplot(df, aes(x, y)) +
  geom_line(size = 1.5, alpha = .5) +
  theme_classic() +
  scale_y_continuous("Função de perda", limits = c(0, 30)) +
  xlab(expression(beta)) +
  geom_segment(data = too_small[1, ], aes(x = x, y = y, xend = x, yend = -Inf), lty = "dashed") +
  geom_point(data = too_small, aes(x, y), size = 3, shape = 21, fill = "blue", alpha = .5) +
  geom_curve(data = too_small, aes(x = x, y = y, xend = x2, yend = y2), curvature = 1, lty = "dotted") +
  theme(
    axis.ticks = element_blank(),
    axis.text = element_blank()
  ) +
  annotate("text", x = df[5, "x"], y = 1, label = "Valor inicial", hjust = -0.1, vjust = 5) +
  ggtitle("b) muito pequeno")
# create too large of a learning rate
too_large <- df[round(which.min(df$y) * (1 + c(-.9, -.6, -.2, .3)), 0), ] %>%
  mutate(x2 = lag(x), y2 = lag(y))
# plot
p2 <- ggplot(df, aes(x, y)) +
  geom_line(size = 1.5, alpha = .5) +
  theme_classic() +
  scale_y_continuous("Função de perda", limits = c(0, 30)) +
  xlab(expression(beta)) +
  geom_segment(data = too_large[1, ], aes(x = x, y = y, xend = x, yend = -Inf), lty = "dashed") +
  geom_point(data = too_large, aes(x, y), size = 3, shape = 21, fill = "blue", alpha = .5) +
  geom_curve(data = too_large, aes(x = x, y = y, xend = x2, yend = y2), curvature = 1, lty = "dotted") +
  theme(
    axis.ticks = element_blank(),
    axis.text = element_blank()
  ) +
  annotate("text", x = too_large[1, "x"], y = 1, label = "Valor inicial", hjust = -0.1, vjust = 5) +
  ggtitle("a) muito grande")
gridExtra::grid.arrange(p2, p1, nrow = 1)
```

Vamos ilustrar utilizando o exemplo acima. Regressão Linear.

$$\hat{L}(f_\beta) = \frac{1}{n} \sum_{i=1}^{n} (x\beta - y)^2 =  \frac{1}{n} || X\beta - Y||_2^2$$

O gradiente é igual a:

\begin{align}
 &\nabla_\beta \hat{L}(f_\beta) &=\\
 & \nabla_\beta \frac{1}{n} || X\beta - Y||_2^2 &=\\
 & \nabla_\beta [(X\beta - Y)'(X\beta - Y)] &=\\
 & \nabla_\beta [(\beta'X' - Y')(X\beta - Y)] &=\\
 & \nabla_\beta [\beta'X'X\beta - 2\beta'X'Y + Y'Y] &=\\
 & 2X'X\beta - 2X'Y &=\\
 & 2X'( X\beta - Y)
\end{align}

## Regressão Logística

Para problemas de classificação binária, nossa variável Y assume dois possíveis valores: 0 ou 1 (sim ou não). Assim, desejamos saber a probabilidade de y assumir o valor target 1. Para isso, o modelo de Regerssão Logística utiliza a função de ligação logística, como função de ligação na combinação linear do modelo.  Agora estamos modelando uma probabilidade, então nossa variável target $Y = p(X)$ indica a probabilidade de ocorrência do evento. 

Então, temos que:

\begin{align}
&p(X) &\ = \quad &\frac{e^{\beta_0 + \beta_1x}}{1 + e^{\beta_0 + \beta_1x}} \\
&\frac{p(X)}{1-p(x)} &\ = \quad &e^{\beta_0 + \beta_1x}
\end{align}

Então:

\begin{align}
& \text{log}\left(\frac{p(X)}{1-p(x)}\right) = {\beta_0 + \beta_1x}
(\#eq:reglogistica)
\end{align}

O processo de otimização para encontrar os estimadores dos parâmetros $\boldsymbol{\beta}$ envolve maximizar a log-verossimilhança da distribuição $Bernoulli(p)$.

> $\text{Bernoulli}(p):$ 
> Distribuição associada a um único experimento aleatório com apenas dois resultados possíveis: $0$, caso se observe $A$; e $1$, caso se observe $A^c$.
> Seja $X=$ número de sucessos (espaço amostral finito), então temos que:

$$f(x)=P(X=x)=\begin{cases} 1-p,&\quad x=0,\\ p,&\quad x=1.\end{cases}$$

<br>

\begin{align}
& L(\beta) &\ = &\prod p_i^{y_i} \times (1-p_i)^{(1-y_i)} \\
& \text{log}L(\beta) &\ = &\sum y_i \text{log}(p_i) + (1-y_i)\text{log}(1-p_i) \\
& \text{log}L(\beta) &\ = &\sum y_i \text{log}(p_i) + \text{log}(1-p_i) - y_i\text{log}(1-p_i) \\
& \text{log}L(\beta) &\ = &\sum y_i \text{log}(p_i) - y_i\text{log}(1-p_i) + \text{log}(1-p_i) \\
& \text{log}L(\beta) &\ = &\sum y_i \text{log}\left(\frac{p_i}{1-p_i}\right) + \text{log}(1-p_i) \\
& \text{log}L(\beta) &\ = &\sum y_i \text{log}\left(\frac{p_i}{1-p_i}\right) + \text{log}\left(\frac{1}{1+e^{\beta_0 + \beta_1x}}\right)
\end{align}

<br>

\begin{align}
& \text{log}L(\beta) & = \sum y_i \text{log}(\frac{p_i}{1-p_i}) + \text{log}(1) - \text{log}(1+e^{\beta_0 + \beta_1x})
\end{align}

Substituindo $\text{log}\left(\frac{p(X)}{1-p(x)}\right) = {\beta_0 + \beta_1x}$, obtemos a função de $\boldsymbol{\beta}$ que desejamos otimizar:

\begin{align}
\sum_{i=1}^n   \left[ y_i \boldsymbol{\beta}^T  \mathbf{x}_{i}  - \log \left(1 + \exp(  \boldsymbol{\beta}^T \mathbf{x}_{i}  \right)    \right]
(\#eq:verosslogistica)
\end{align}


### Newton-Raphson

Neste caso não possuimos uma solução fechada para ${\bf \beta}$. O método Newton-Raphson é um método iterativo que pode ser usado para estimar o vetor de parâmetros ${\bf \beta}$. Em cada iteração, fazemos uma aproximação quadrática do log de verossimilhança e maximizamos isso.

Mais especificamente, seja $u \in \mathbb{R}^p$ o gradiente $\partial L / \partial \beta$, ${\bf H} \in \mathbb{R}^{p \times p}$ a matriz Hessiana, e assuma que $\beta^{(k)}$ é nossa estimativa para $\hat{\beta}$ no tempo $k$. (Podemos pensar em $u$ e ${\bf H}$ como funções de $\beta$). Sejam $u^{(t)}$ e ${\bf H}^{(k)}$ a avaliação de $u$ e $\bf H$ em $\beta^{(k)}$. A expansão de Taylor de $L$ em torno de $\beta^{(k)}$ dá

\begin{aligned} L(\beta) \approx L(\beta^{(t)}) + (u^{(t)})^T (\beta - \beta^{(t)}) + \frac{1}{2} (\beta - \beta^{(t)})^T {\bf H}^{(t)} (\beta - \beta^{(t)}). \end{aligned}

Derivando em relação à $\beta$ e definindo-o como 0, podemos reorganizar os termos para resolver $\beta$, dando-nos a próxima estimativa

O vetor score ${\bf u}$ e a matriz Hessiana ${\bf H}$ representam as derivadas de primeira e segunda ordem, respectivamente, em que:

\begin{align}
\frac{\partial l}{\partial \beta}  =\boldsymbol{X}^T\boldsymbol{(y - p)}\\
%\frac{\partial ^2 \ell}{\partial \beta^2} = - \sum_{i=1}^n \mathbf{x}_{i} \mathbf{x}_{i}^T p_i (1-p_i) \\
\frac{\partial ^2 l}{\partial \beta^2} = \boldsymbol{-X^TWX } 
\end{align}

em que $\boldsymbol{W}$ é uma matriz diagonal $n \times n$ com o i-ésimo elemento dado por $p_i(1-p_i)$, ou seja, $W_{ii} = p_i (1-p_i)$, com $1 \le i \le n$.
. Dessa maneira, o algoritmo pode ser reescrito como:

\begin{aligned} \beta^{(t+1)} = \beta^{(t)} - ({\bf H}^{(t)})^{-1} u^{(t)}. \end{aligned}


\begin{align}
\boldsymbol{\beta}^{(k+1)} = \boldsymbol{\beta}^{(k)} - \left(  \frac{\partial ^2 l}{\partial \beta^2} \right) \frac{\partial l}{\partial \beta}
\end{align}


\begin{align}
\boldsymbol{\beta}^{(k+1)} = \boldsymbol{\beta}^{(k)} + (\mathbf{X}^T\mathbf{WX})^{-1}\mathbf{X}^T\mathbf{(y-p)} 
\end{align}