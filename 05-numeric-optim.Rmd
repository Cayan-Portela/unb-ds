# Otimização

## Estatística e Machine Learning

A abordagem estatística para análise e resumo de informações contidas em um conjunto de dados, consiste na suposição de que existe um mecanismo estocástico gerador do processo em análise [@bonat2012metodos]. Para isso, existem técnica que nos auxiliam na estimação de uma função (e.g. de regressão) que são de fundamental importância em estatística e machine learning.

Muitas ténicas de regressão usadas hoje são datadas de muitos anos atrás. Todavia, com o avanço computacional, métodos mais robustos em relação ao real processo gerador vêm ganhando cada vez mais espaço e importância[@izbicki2020aprendizado]. Por exemplo, em modelos com mais covariáveis que observações, métodos tradicionais sofrem pela falta de graus de liberdade.

De maneira geral, nosso objetivo é determinar uma relação entre uma variável aleatória de interesse $Y$ e um vetor de covariáveis $x = (x_1, ..., x_n)$. Então, temos que:

$$ g(x) := E [ Y | X = x]$$ 


## Regressão Linear

Apesar de simples, a regressão linear é um modelo de extrema importância. Um bom entendimento é resposável por fundamentos sólidos em conceitos de modelagem. Em um modelo de regressão linear, temos as seguintes definições:

\begin{align}
  y_i = \beta_0 + \beta_1 x_i + \epsilon_i (\#eq:um)
\end{align}

- $y_i$ é a variávei de dependente e $x_i$ é a variável explicativa.

- $\beta_0$ e $\beta_1$ são os parâmetros a serem estimados pelo modelo.

- $\epsilon_i$ é o erro associdao à i-ésima observação.

Vamos considerar um exemplo utilizando os dados 'mtcars', que consiste em informações sobre automóveis.
```{r, echo=FALSE, message=FALSE, warnings=FALSE}
data(mtcars)
knitr::kable(head(mtcars))
```
Quanto mais pesado o carro, menor a autonomia (quantidade de milhas percorridas por galão)?

$$Autonomia_i = \beta_0 + \beta_1 Peso_i + \epsilon_i$$

```{r plot-last2, echo=FALSE, message=FALSE, warnings=FALSE}
library(ggplot2)

mtcars$pred <- lm(mpg ~ wt, data = mtcars)$fitted.values

ggplot(data = mtcars, aes(x = wt, y = mpg)) +
  labs(x = "Peso", y = "Milhas p/ galão") +
  geom_segment(aes(xend = wt, yend = pred), alpha = .4, col = "firebrick") +  # alpha 
  geom_smooth(method="lm", se = FALSE) +
  geom_point(size = 2.5, col = "cyan4") +
  #geom_point(aes(y = pred), shape = 1) +
  ylim(c(8,34)) +
  theme_minimal()
```

Para encontrar a reta que melhor se ajusta aos pontos utilizamos a abordagem de mínimos quadrados, que consiste em encontrar estimadores $\hat{\beta_0}$ e $\hat{\beta_1}$ que minimizam a soma dos erros quadráticos. 

$$y_i = \beta_0 + \beta_1 x_i + \epsilon_i$$
$$\epsilon_i = y_i - \beta_0 - \beta_1 x_i$$
$$\epsilon_i^2 = (y_i - \beta_0 - \beta_1 x_i)^2$$
$$\sum_{i = 1}^{n} \epsilon_i^{2} = \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2$$

Temos então, nossa função de custo, que queremos minimzar:

\begin{align}
  \sum_{i = 1}^{n} \epsilon_i^{2} = \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2 (\#eq:dois)
\end{align}

### Regressão Linear Simples

Casos em que temos apenas uma covariável (x) como variável explicativa (como o exemplo acima), dizemos estar em uma regressão linear simples.

Para minimizar a função, aplicamos derivadas parciais, em respeito a $\hat{\beta_0}$ e $\hat{\beta_1}$, e igualamos a zero.
Como minimizar uma função?

Para minimizar uma função $f(x)$, calculamos sua derivada e igualamos a zero. Para verificar que $f'(x) = 0$ é um ponto de mínimo, devemos observar o sinal da segunda derivada, $f''(x)$.

![](images/simple_tangent_graph.png)

Fazendo as derivadas parciais da função de custo (L) em relação a $\beta_0$ e em relação a $\beta_1$, temos:

$$L = \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2$$
\begin{align}
\frac{\partial L}{\partial \beta_0} = -2 \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i) (\#eq:tres)
\end{align}

\begin{align}
\frac{\partial L}{\partial \beta_1} = -2 \sum_{i=1}^{n} x_i(y_i - \beta_0 - \beta_1 x_i) (\#eq:cinco)
\end{align}


Igualando $\frac{\partial L}{\partial \beta_0} = 0$ e $\frac{\partial L}{\partial \beta_1} = 0$, obtemos as equações:

\begin{align}
  \sum_{i=1}^{n} (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0	\rightarrow n \hat{\beta}_0 + \hat{\beta}_1 \sum_{i=1}^{n} x_i = \sum_{i=1}^{n} y_i (\#eq:seis)
\end{align}

\begin{align}
  \sum_{i=1}^{n} x_i(y_i - \beta_0 - \beta_1 x_i) = 0	\rightarrow \hat{\beta}_0 \sum_{i=1}^{n} x_i + \hat{\beta}_1 \sum_{i=1}^{n} x_i^2 = \sum_{i=1}^{n} x_i y_i (\#eq:sete)
\end{align}

De \@ref(eq:seis) temos então:

\begin{align}
\hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x}
(\#eq:b0)
\end{align}

Substituindo \@ref(eq:b0) em \@ref(eq:sete), temos:

$$
\hat{\beta_1}  = \frac{\sum_{i=1}^{n} x_i y_i - \frac{1}{n}\sum_{i=1}^{n}x_i \sum_{i=1}^{n} y_i  }{\sum_{i=1}^{n}x^2 - \frac{1}{n} (\sum_{i=1}^{n}x_i)^2} = 
\frac{n \sum_{i=1}^{n} x_i y_i - \sum_{i=1}^{n} x_i \sum_{i=1}^{n} y_i}{n \sum_{i=1}^{n} x_i^2 - (\sum_{i=1}^{n} x_i)^2}
$$

\begin{align}
\hat{\beta_1} = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^2} (\#eq:b1)
\end{align}


---
Exercício. Fazer "na mão", comparando com o resultado da função lm().

### Forma Matricial

O modelo visto acima pode ser representado matricialmente, generalizando para o caso em que temos mais de uma variável explicativa. 

$$y_1 = \beta_0 + \beta_1 x_1 + \epsilon_1$$
$$y_2 = \beta_0 + \beta_1 x_2 + \epsilon_2$$
$$\vdots$$
$$y_n = \beta_0 + \beta_1 x_n + \epsilon_n$$

$$\begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n  \end{bmatrix} = \begin{bmatrix} 1 & x_{11}\\ 1 & x_{12} \\ \vdots & \vdots \\ 1 & x_{1n}  \end{bmatrix} \times \begin{bmatrix} \beta_0 \\ \beta_1 \\  \end{bmatrix} + \begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n  \end{bmatrix}$$

$$\Large \boldsymbol{Y} = \boldsymbol{XB} + \boldsymbol{\epsilon}$$

Como estimar os parâmetros com a representação matricial? Mesma idéia: minimizar os erros!

$$\boldsymbol{Y} = \boldsymbol{X\beta} + \boldsymbol{\epsilon}$$
$$\boldsymbol{\epsilon} = \boldsymbol{Y} - \boldsymbol{X\beta}$$

Mínimos quadrados: como representar soma quadrática dos erros matricialmente?
$$\boldsymbol{\epsilon}' \boldsymbol{\epsilon}$$
Então,

$$\boldsymbol{\epsilon}' \boldsymbol{\epsilon} = (\boldsymbol{Y} - \boldsymbol{X\beta})'(\boldsymbol{Y} - \boldsymbol{X\beta})$$

Novamente: mesma ideia, derivar e igualar a zero.

$$\boldsymbol{Y} = \boldsymbol{X\beta} + \boldsymbol{\epsilon}$$
$$\boldsymbol{\epsilon} = \boldsymbol{Y} - \boldsymbol{X\beta}$$
$$\boldsymbol{\epsilon}' \boldsymbol{\epsilon} = (\boldsymbol{Y} - \boldsymbol{X\beta})'(\boldsymbol{Y} - \boldsymbol{X\beta})$$
$$= (\boldsymbol{Y} - \boldsymbol{X\beta})'(\boldsymbol{Y} - \boldsymbol{X\beta})$$
$$= (\boldsymbol{Y'} - \boldsymbol{\beta'X'})(\boldsymbol{Y} - \boldsymbol{X\beta})$$
$$= \boldsymbol{Y'}Y - \boldsymbol{\beta'X'Y} - \boldsymbol{Y'X\beta} + \boldsymbol{\beta'X'X\beta}$$
$$= \boldsymbol{Y'Y} - 2\boldsymbol{\beta'X'Y}  + \boldsymbol{\beta'X'X\beta}$$
$$-2\boldsymbol{X'Y}  + 2\boldsymbol{X'X\beta} = 0$$
$$\boldsymbol{X'X\beta} = \boldsymbol{X'Y}$$
$$\boldsymbol{\hat{\beta}} = \boldsymbol{(X'X)^{-1}X'Y}$$

### Decomposição QR

Operações envolvendo a inversa de uma matriz podem ser bastante custosas do ponto de vista computacional, principalmente quando a matriz cresce em dimensões. Uma alternativa bastante utilizada é a decomposição $QR$.

Sendo $X_{n\times p}$ uma matriz de posto completo ($n$ > $p$, rank=$p$), então podemos decompor $X$ em $$X = QR$$

em que:

- $Q$ tem dimensões $n\times p$ e $Q'Q = I$
- $R$ tem dimensões $p \times p$ e é uma matriz triangular superior 

Dessa maneira, podemos reescrever o processo de mínimos quadrados acima, como:

$$\boldsymbol{X'X\beta} = \boldsymbol{X'Y}$$
$$\boldsymbol{(QR)'(QR)\beta} = \boldsymbol{(QR)'Y}$$
$$\boldsymbol{R'(Q'Q)R)\beta} = \boldsymbol{R'Q'Y}$$
$$\boldsymbol{R'R\beta} = \boldsymbol{R'Q'Y}$$
$$\boldsymbol{(R')^{-1}R'R\beta} = \boldsymbol{(R')^{-1}R'Q'Y}$$
$$\boldsymbol{R\beta} = \boldsymbol{Q'Y}$$

> Exercícios: adicione colinearidade na regressão dos dados mtcars e compare o resultado da função `lm()` no R e `sklearn.linear_model.LinearRegression()` do python.

## Gradiente Descendente
