# Otimização

## Estatística e Machine Learning

A abordagem estatística para análise e resumo de informações contidas em um conjunto de dados, consiste na suposição de que existe um mecanismo estocástico gerador do processo em análise [@bonat2012metodos]. Para isso, existem técnica que nos auxiliam na estimação de uma função (e.g. de regressão) que são de fundamental importância em estatística e machine learning.

Muitas ténicas de regressão usadas hoje são datadas de muitos anos atrás. Todavia, com o avanço computacional, métodos mais robustos em relação ao real processo gerador vêm ganhando cada vez mais espaço e importância[@izbicki2020aprendizado]. Por exemplo, em modelos com mais covariáveis que observações, métodos tradicionais sofrem pela falta de graus de liberdade.

De maneira geral, nosso objetivo é determinar uma relação entre uma variável aleatória de interesse $Y$ e um vetor de covariáveis $x = (x_1, ..., x_n)$. Então, temos que:

$$ g(x) := E [ Y | X = x]$$ 


## Regressão Linear

_Regressão Linear_ configura um dos algoritmos mais simples em aprendizado supervisionado, porém, é um modelo de extrema importância. Além de ser bastante eficaz em diversas aplicações, seu bom entendimento é resposável por fundamentos sólidos em conceitos de modelagem, pois diversas modelagens mais sofisticadas podem ser vistas como generalizações de uma regressão linear. Esta seção apresenta alguns conceitos de regressão linear. Um estudo extremamente rico compreensivo no assunto pode ser visto em @kutner2005applied.

Em um modelo de regressão linear simples, temos a seguinte relação:

\begin{align}
  y_i = \beta_0 + \beta_1 x_i + \epsilon_i \qquad  \qquad i = 1, ..., n (\#eq:um)
\end{align}

em que $y_i$ é a variávei de interesse (dependente), $x_i$ é a variável explicativa (independente), $\beta_0$ e $\beta_1$ são os parâmetros a serem estimados pelo modelo e $\epsilon_i$ representa o erro associdao à i-ésima observação.

Vamos considerar um exemplo utilizando os dados `mtcars` (disponível em `R`), que consiste em informações sobre automóveis.
```{r, echo=FALSE, message=FALSE, warning=FALSE}
data(mtcars)
knitr::kable(head(mtcars, 3))
```
Quanto mais pesado o carro, menor a autonomia (quantidade de milhas percorridas por galão)?

$$Autonomia_i = \beta_0 + \beta_1 Peso_i + \epsilon_i$$

```{r plot-last2, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)

mtcars$pred <- lm(mpg ~ wt, data = mtcars)$fitted.values

ggplot(data = mtcars, aes(x = wt, y = mpg)) +
  labs(x = "Peso", y = "Milhas p/ galão") +
  geom_segment(aes(xend = wt, yend = pred), alpha = .4, col = "firebrick") +  # alpha 
  geom_smooth(method="lm", se = FALSE) +
  geom_point(size = 2.5, col = "cyan4") +
  #geom_point(aes(y = pred), shape = 1) +
  ylim(c(8,34)) +
  theme_minimal()
```

Para encontrar a reta que melhor se ajusta aos pontos utilizamos a abordagem de mínimos quadrados, que consiste em encontrar estimadores $\hat{\beta_0}$ e $\hat{\beta_1}$ que minimizam a soma dos erros quadráticos. 

$$y_i = \beta_0 + \beta_1 x_i + \epsilon_i$$
$$\epsilon_i = y_i - \beta_0 - \beta_1 x_i$$
$$\epsilon_i^2 = (y_i - \beta_0 - \beta_1 x_i)^2$$
$$\sum_{i = 1}^{n} \epsilon_i^{2} = \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2$$

Temos então, nossa função de custo, que queremos minimzar:

\begin{align}
  \sum_{i = 1}^{n} \epsilon_i^{2} = \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2 (\#eq:dois)
\end{align}

### Regressão Linear Simples

Casos em que temos apenas uma covariável (x) como variável explicativa (como o exemplo acima), dizemos estar em uma regressão linear simples.

Para minimizar a função, aplicamos derivadas parciais, em respeito a $\hat{\beta_0}$ e $\hat{\beta_1}$, e igualamos a zero.
Como minimizar uma função?

Para minimizar uma função $f(x)$, calculamos sua derivada e igualamos a zero. Para verificar que $f'(x) = 0$ é um ponto de mínimo, devemos observar o sinal da segunda derivada, $f''(x)$.

![](images/simple_tangent_graph.png)

Fazendo as derivadas parciais da função de custo (L) em relação a $\beta_0$ e em relação a $\beta_1$, temos:

$$L = \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2$$
\begin{align}
\frac{\partial L}{\partial \beta_0} = -2 \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i) (\#eq:tres)
\end{align}

\begin{align}
\frac{\partial L}{\partial \beta_1} = -2 \sum_{i=1}^{n} x_i(y_i - \beta_0 - \beta_1 x_i) (\#eq:cinco)
\end{align}


Igualando $\frac{\partial L}{\partial \beta_0} = 0$ e $\frac{\partial L}{\partial \beta_1} = 0$, obtemos as equações:

\begin{align}
  \sum_{i=1}^{n} (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0	\rightarrow n \hat{\beta}_0 + \hat{\beta}_1 \sum_{i=1}^{n} x_i = \sum_{i=1}^{n} y_i (\#eq:seis)
\end{align}

\begin{align}
  \sum_{i=1}^{n} x_i(y_i - \beta_0 - \beta_1 x_i) = 0	\rightarrow \hat{\beta}_0 \sum_{i=1}^{n} x_i + \hat{\beta}_1 \sum_{i=1}^{n} x_i^2 = \sum_{i=1}^{n} x_i y_i (\#eq:sete)
\end{align}

De \@ref(eq:seis) temos então:

\begin{align}
\hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x}
(\#eq:b0)
\end{align}

Substituindo \@ref(eq:b0) em \@ref(eq:sete), temos:

$$
\hat{\beta_1}  = \frac{\sum_{i=1}^{n} x_i y_i - \frac{1}{n}\sum_{i=1}^{n}x_i \sum_{i=1}^{n} y_i  }{\sum_{i=1}^{n}x^2 - \frac{1}{n} (\sum_{i=1}^{n}x_i)^2} = 
\frac{n \sum_{i=1}^{n} x_i y_i - \sum_{i=1}^{n} x_i \sum_{i=1}^{n} y_i}{n \sum_{i=1}^{n} x_i^2 - (\sum_{i=1}^{n} x_i)^2}
$$

\begin{align}
\hat{\beta_1} = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^2} (\#eq:b1)
\end{align}


---
Exercício. Fazer "na mão", comparando com o resultado da função lm().

### Forma Matricial

O modelo visto acima pode ser representado matricialmente, generalizando para o caso em que temos mais de uma variável explicativa. 

$$y_1 = \beta_0 + \beta_1 x_1 + \epsilon_1$$
$$y_2 = \beta_0 + \beta_1 x_2 + \epsilon_2$$
$$\vdots$$
$$y_n = \beta_0 + \beta_1 x_n + \epsilon_n$$

$$\begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n  \end{bmatrix} = \begin{bmatrix} 1 & x_{11}\\ 1 & x_{12} \\ \vdots & \vdots \\ 1 & x_{1n}  \end{bmatrix} \times \begin{bmatrix} \beta_0 \\ \beta_1 \\  \end{bmatrix} + \begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n  \end{bmatrix}$$

$$\Large \boldsymbol{Y} = \boldsymbol{XB} + \boldsymbol{\epsilon}$$

Como estimar os parâmetros com a representação matricial? Mesma idéia: minimizar os erros!

$$\boldsymbol{Y} = \boldsymbol{X\beta} + \boldsymbol{\epsilon}$$
$$\boldsymbol{\epsilon} = \boldsymbol{Y} - \boldsymbol{X\beta}$$

Mínimos quadrados: como representar soma quadrática dos erros matricialmente?
$$\boldsymbol{\epsilon}' \boldsymbol{\epsilon}$$
Então,

$$\boldsymbol{\epsilon}' \boldsymbol{\epsilon} = (\boldsymbol{Y} - \boldsymbol{X\beta})'(\boldsymbol{Y} - \boldsymbol{X\beta})$$

Novamente: mesma ideia, derivar e igualar a zero.

$$\boldsymbol{Y} = \boldsymbol{X\beta} + \boldsymbol{\epsilon}$$
$$\boldsymbol{\epsilon} = \boldsymbol{Y} - \boldsymbol{X\beta}$$
$$\boldsymbol{\epsilon}' \boldsymbol{\epsilon} = (\boldsymbol{Y} - \boldsymbol{X\beta})'(\boldsymbol{Y} - \boldsymbol{X\beta})$$
$$= (\boldsymbol{Y} - \boldsymbol{X\beta})'(\boldsymbol{Y} - \boldsymbol{X\beta})$$
$$= (\boldsymbol{Y'} - \boldsymbol{\beta'X'})(\boldsymbol{Y} - \boldsymbol{X\beta})$$
$$= \boldsymbol{Y'}Y - \boldsymbol{\beta'X'Y} - \boldsymbol{Y'X\beta} + \boldsymbol{\beta'X'X\beta}$$
$$= \boldsymbol{Y'Y} - 2\boldsymbol{\beta'X'Y}  + \boldsymbol{\beta'X'X\beta}$$
$$-2\boldsymbol{X'Y}  + 2\boldsymbol{X'X\beta} = 0$$
$$\boldsymbol{X'X\beta} = \boldsymbol{X'Y}$$
$$\boldsymbol{\hat{\beta}} = \boldsymbol{(X'X)^{-1}X'Y}$$

### Decomposição QR

Operações envolvendo a inversa de uma matriz podem ser bastante custosas do ponto de vista computacional, principalmente quando a matriz cresce em dimensões. Uma alternativa bastante utilizada é a decomposição $QR$ (através do algoritmo Gram-Schmidt, por exemplo).

Sendo $X_{n\times p}$ uma matriz de posto completo ($n$ > $p$, rank=$p$), então podemos decompor $X$ em $$X = QR$$

em que:

- $Q$ tem dimensões $n\times p$ e $Q'Q = I$
- $R$ tem dimensões $p \times p$ e é uma matriz triangular superior 

Dessa maneira, podemos reescrever o processo de mínimos quadrados acima, como:



\begin{align}
&\boldsymbol{X'X\beta} &\ = \quad &\boldsymbol{X'Y} \\
&\boldsymbol{(QR)'(QR)\beta} &\ = \quad  &\boldsymbol{(QR)'Y} \\
&\boldsymbol{R'R\beta} &\ = \quad  &\boldsymbol{R'Q'Y} \\
&\boldsymbol{(R')^{-1}R'R\beta} &\ = \quad & \boldsymbol{(R')^{-1}R'Q'Y} \\
&\boldsymbol{R\beta} &\ = \quad & \boldsymbol{Q'Y} \\
\end{align}

$$\boldsymbol{R\beta} = \boldsymbol{Q'Y}$$

Dessa maneira, podemos chegar nos valores preditos  $\hat{y}$ através de $\boldsymbol{\hat{Y}} = \boldsymbol{QRR^{-1}Q'} = \boldsymbol{QQ'Y}$

> **_Exercício_**: adicione colinearidade na regressão dos dados mtcars e compare o resultado da função `lm()` no R e `sklearn.linear_model.LinearRegression()` do python.

## Gradiente Descendente


Gradiente descendente . . . 

```{r gradient-descent-fig, warning=FALSE, message=FALSE, echo=FALSE, fig.height=4, fig.width=7, fig.align = 'center', fig.cap="Gradiente descendente minimiza a função de custo gradualmente, ajustando o(s) parâmetro(s) em cada passo (figura adaptada de Boehmke & Greenwell, 2019)."}

# create data to plot
x <- seq(-5, 5, by = .05)
y <- x^2 + 3
df <- data.frame(x, y)
step <- 5
step_size <- .2
for(i in seq_len(18)) {
  next_step <- max(step) + round(diff(range(max(step), which.min(df$y))) * step_size, 0)
  step <- c(step, next_step)
  next
}
steps <- df[step, ] %>%
  mutate(x2 = lag(x), y2 = lag(y)) %>%
  dplyr::slice(1:18)
# plot
ggplot(df, aes(x, y)) +
  geom_line(size = 1.5, alpha = .5) +
  theme_classic() +
  scale_y_continuous("Função de perda", limits = c(0, 30)) +
  xlab(expression(beta)) +
  geom_segment(data = df[c(5, which.min(df$y)), ], aes(x = x, y = y, xend = x, yend = -Inf), lty = "dashed") +
  geom_point(data = filter(df, y == min(y)), aes(x, y), size = 4, shape = 21, fill = "yellow") +
  geom_point(data = steps, aes(x, y), size = 3, shape = 21, fill = "blue", alpha = .5) +
  geom_curve(data = steps, aes(x = x, y = y, xend = x2, yend = y2), curvature = 1, lty = "dotted") +
  theme(
    text = element_text(size=15),
    axis.ticks = element_blank(),
    axis.text = element_blank()
  ) +
  annotate("text", x = df[5, "x"], y = 1, label = "Valor inicial", hjust = -0.1, vjust = .8, size=5) +
  annotate("text", x = df[which.min(df$y), "x"], y = 1, label = "Mínimo", hjust = -0.1, vjust = .8, size=5) +
  annotate("text", x = df[1, "x"]-0.75, y = df[10, "y"], label = "Taxa de aprendizado", hjust = -.8, vjust = 0, size=5)
```